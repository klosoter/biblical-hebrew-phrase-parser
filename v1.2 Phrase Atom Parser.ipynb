{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Neural Network to Build a Phrase Atom Parser for Biblical Hebrew\n",
    "\n",
    "by Mark Klooster | October 2, 2020 | ETCBC\n",
    "\n",
    "Original can be found here: http://etcbc.nl/bible/using-a-neural-network-to-build-a-phrase-atom-parser-for-biblical-hebrew/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook publishes the results of my internship the ETCBC (Eep Talstra Centre for Bible and Computer). The internship project involved creating a phrase atom parser for Hebrew text by building a machine learning model. The phrase atom parser contributes to the joint project between the ETCBC and the Theological Seminary at Andrews University, which is called the Creating Annotated Corpora of Classical Hebrew Texts project (CACCHT). The CACCHT project is currently broadening their scope by adding more text corpora to their database. A few years ago, the research group created a new Text-Fabric module containing the Dead Sea Scrolls (DSS) with morphological encoding. The digitized DSS and their morphological annotations were provided by Martin Abegg. However, Abegg's encoding system is very different from the other modules encoded by the ETCBC (such as the BHSA package or the extra-biblical package). Therefore, the CACCHT project has been working on converting all morphological features, thereby using a bottom-up approach (i.e. converting word features first, then phrase features, clause features, and so on). \n",
    "\n",
    "The encoding of word features is well underway and the project is about to move on to encoding phrase atom features. To start this, it should first be known which words constitute a phrase atom. However, Abegg's encoding does not have information about phrase atom boundaries in the dataset. Therefore, the phrase atom boundaries have to be constructed first. The construction or prediction of phrase atom boundaries is the project of this notebook. \n",
    "\n",
    "The reason this project predicts *phrase atom* boundaries instead of *phrase* boundaries is that whereas phrases might be separated by words of another phrase, phrase atoms consist of continuous words. Take for example this English sentence:\n",
    "\n",
    "'A clearer example has never been given'.\n",
    "\n",
    "The *adverb* and *adverbial phrase* 'never' split the *verbal phrase* 'has been given' in two smaller *phrase atoms*, namely 'has' and 'been given'. As phrases that are interrupted by other phrases are harder to detect, it is more logical to try and find phrase **atom** boundaries first. This agrees with the bottom-up approach that is used in the CACCHT project. (In another project, phrase atoms found here could be used to find complete phrases).\n",
    "\n",
    "Determining things like phrase atom boundaries used to be done manually. However, this notebook uses a different approach. Phrase atom boundaries will be deducted and predicted based on information on word level. The data set of the BHSA already has information on *all* levels, including phrase atom boundaries, while the DSS data set has only information on word level. The data of the BHSA will be used as training data for a neural network. A neural network is an example of a machine learning algorithm which has a pattern-based approach. This means that rather than feeding rules to an algorithm to predict phrase atom boundaries, the networks will find patterns between input (on word level) and the output (phrase atom boundaries), to come up with these rules itself. These rules, in turn, will be applied on word-level input of the DSS to predict phrase atom boundaries. \n",
    "\n",
    "The neural network will be trained to find statistical patterns between *part of speech* - a word-level feature that has been encoded for the DSS already - and phrase atom boundaries. The deep learning model is trained on 90 per cent of the chapters of the BHSA, of which the phrase atom boundaries (the output) are known. As input, the model takes part of speech (e.g. noun, verb, adjective, etc.). The output consists of a 'p' or an 'x', indicating, respectively, whether the word is the end of a phrase atom, or not. \n",
    "\n",
    "The trained model is then tested on the remaining 10 per cent of the BHSA, which is called the test set. The mistakes are evaluated in detail, to get insight into specific cases in which the model is incorrect. The evaluation has led to several alterations in the input data, which in turn have improved the accuracy of the model. The model below only shows the final script of the most accurate model. The following alterations were made based on the evaluation of simpler models:\n",
    "\n",
    "1. The scope of the training set was limited to Hebrew words only. This means that Aramaic parts were left out. As Aramaic has different grammatical conventions, it would not help the prediction of phrase atom boundaries in Hebrew. For example, in Aramaic, the part of speech 'article' comes after the noun, while it precedes the noun in Hebrew. Therefore, in Aramaic, the article would often be the last word of a phrase atom, while this would be theoretically impossible in Hebrew. Moreover, the target scroll of the DSS for this experiment, the Qumran Community Scroll (1QS), is a Hebrew text.\n",
    "2. According to the Abegg encoding and the lastest ETCBC convention (applied in the extra-biblical package but not in the BHSA package), pronominal suffixes are to be treated as separate, individual words. Therefore, in the pre-processing phase of the data of the BHS, all suffixes are separated from their base words. In some cases, the suffix formed a separate, individual phrase atom. Suppose, for example, a verb with an object suffix. The verb and the suffix are in reality separate phrase atoms (the verb is the predicate, while the suffix is the object of the broader clause). For those and similar cases the output (phrase atom boundary) is re-evaluated and adjusted accordingly. \n",
    "3. Previous models were especially inaccurate when the part of speech was a noun, adverb, verb, proper noun, or an adjective. Interestingly, these five parts of speech can have a construct state in Hebrew, in which case it is closely connected to the following word. As a result of this, these cases are rarely the end of a phrase atom. Adding extra information about the state to these parts of speeches helps the model to predict more accurate in these cases.\n",
    "These alterations combined made the accuracy of the model on the test set of the BHSA jump from 89 per cent to 97 per cent. The accuracy of the most efficient model on 1QS reached almost 95 per cent.\n",
    "\n",
    "Moreover, whether a word is the end of a phrase atom or not, cannot be deducted from its part of speech alone. When dealing with language, context is crucial. Therefore, as is common in the practice of natural language processing, the model works with input and output *sequences* instead of single input and output. This is called a sequence to sequence model (seq2seq). After testing sequence lengths ranging from 5 to 20, the most ideal and efficient sequence length was 9. Therefore, the model works with sequences of length 9. This means that the input consists of 9 consecutive parts of speech and the output of 9 phrase atom boundary indicators (x’s or p’s).\n",
    "\n",
    "In the script below, the following steps are taken:\n",
    "1.\tThe input and output data are collected and pre-processed for the network.\n",
    "2.\tThe network is defined, compiled, and fit to the training data.\n",
    "3.\tThe model’s performance on the test set is calculated and evaluated extensively.\n",
    "4.\tThe input and output data for the DSS scroll (1QS) is collected and pre-processed.\n",
    "5.\tThe model is run on 1QS and the results are evaluated.\n",
    "\n",
    "\n",
    "Each step is explained in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the necessary libraries and modules are imported. This includes the Tensorflow package to build neural networks and the Text-Fabric package containing the BHSA database. \n",
    "\n",
    "It is recommended to run the model on a GPU instead on a CPU because that is much faster (depending on the specifications of the GPU of course). In order to do this, a virtual environment needs to be created. This might be a bit complicated but there are various good explanations and tutorials available online. See, for example, this tutorial on how to install a Tensorflow-GPU: https://www.youtube.com/watch?v=tPq6NIboLSc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate limit is 5000 requests per hour, with 5000 left for this hour\n",
      "\tconnecting to online GitHub repo annotation/app-bhsa ... connected\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b title=\"latest commit\">TF-app:</b> <span title=\"rv2.3.0=#113c0687cfce3077734dac1844d244d20f4ace6f\">C:\\Users\\Mark/text-fabric-data/annotation/app-bhsa/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv1.6=#bac4a9f5a2bbdede96ba6caea45e762fe88f88c5 offline under ~/text-fabric-data\">C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2=#1ac68e976ee4a7f23eb6bb4c6f401a033d0ec169 offline under ~/text-fabric-data\">C:\\Users\\Mark/text-fabric-data/etcbc/phono/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2=#395dfe2cb69c261862fab9f0289e594a52121d5c offline under ~/text-fabric-data\">C:\\Users\\Mark/text-fabric-data/etcbc/parallels/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 8.3.0</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa TF-app\">app-bhsa</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/0_home\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Parallel Passages</b></summary><b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/parallels/tf/c/crossref.tf\">crossref</a></i></b><br></details><details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/book.tf\">book</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/book@am.tf\">book@ll</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/code.tf\">code</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/det.tf\">det</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/domain.tf\">domain</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/function.tf\">function</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/g_cons.tf\">g_cons</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/g_cons_utf8.tf\">g_cons_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/g_lex.tf\">g_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/g_lex_utf8.tf\">g_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/g_word.tf\">g_word</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/g_word_utf8.tf\">g_word_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/label.tf\">label</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/language.tf\">language</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/lex.tf\">lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/lex_utf8.tf\">lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/ls.tf\">ls</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/nametype.tf\">nametype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/nme.tf\">nme</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/number.tf\">number</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/pargr.tf\">pargr</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/pdp.tf\">pdp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/pfm.tf\">pfm</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/prs.tf\">prs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/prs_gn.tf\">prs_gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/prs_nu.tf\">prs_nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/prs_ps.tf\">prs_ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/ps.tf\">ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer.tf\">qere_trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer_utf8.tf\">qere_trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/qere_utf8.tf\">qere_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/rank_lex.tf\">rank_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/rela.tf\">rela</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/sp.tf\">sp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/st.tf\">st</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/tab.tf\">tab</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/trailer.tf\">trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/trailer_utf8.tf\">trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/txt.tf\">txt</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/typ.tf\">typ</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/uvf.tf\">uvf</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/vbe.tf\">vbe</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/vbs.tf\">vbs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/voc_lex.tf\">voc_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/voc_lex_utf8.tf\">voc_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/vs.tf\">vs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/vt.tf\">vt</a><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/mother.tf\">mother</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/bhsa/tf/c/oslots.tf\">oslots</a></i></b><br></details><details><summary><b>Phonetic Transcriptions</b></summary><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/phono/tf/c/phono.tf\">phono</a><br><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\Mark/text-fabric-data/etcbc/phono/tf/c/phono_trailer.tf\">phono_trailer</a><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 0.1rem;\n",
       "    margin: 0.1rem;\n",
       "    direction: ltr;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1rem 0rem;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".section {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--section);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  0.5rem 0.1rem 0.1rem 0.1rem;\n",
       "    margin: 0.8rem 0.1rem 0.1rem 0.1rem;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 0.2rem;\n",
       "    margin-left: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 0.2rem;\n",
       "    margin-right: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -1.2rem;\n",
       "    margin-left: 1rem;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3rem;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 0.1rem;\n",
       "    margin-left: 0.1rem;\n",
       "    padding: 0.1rem 0.1rem;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 2rem;\n",
       "\tpadding: 1rem;\n",
       "\tborder: 0.1rem solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--section:            hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         0.15rem;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   0.1rem;\n",
       "  --border-width0:      0.1rem;\n",
       "  --border-width1:      0.15rem;\n",
       "  --border-width2:      0.2rem;\n",
       "  --border-width3:      0.3rem;\n",
       "  --border-width4:      0.25rem;\n",
       "  --border-width-plain: 0.1rem;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 0.2rem ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 0.2rem ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.3rem;\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>Text-Fabric API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"doc\">N F E L T S C TF</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports the necessary libraries and modules\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# imports the ETCBC database of the BHSA\n",
    "from tf.app import use\n",
    "A = use('bhsa:hot', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is important to collect all words of the BHSA that are suitable for this project's purposes. As the ideal sequence length is 9, it is useful to collect these words in ranges longer than the sequence length. Moreover, there has to be a certain amount of ranges so that a random 10 per cent (for the test set) is representative of all genres throughout the Hebrew Bible. Therefore, the entire Hebrew Bible is split up into 929 smaller blocks containing consecutive words from exactly one chapter. The next step is to delete all words that are not Hebrew but Aramaic (to get a more homogeneous dataset). This is done in two steps:\n",
    "1. Each block is checked for the presence of Aramaic words, which are then deleted.\n",
    "2. At the position of the gap left by the deleted Aramaic words, the block is split into two new blocks (or more if the block has multiple gaps). \n",
    "\n",
    "This way, the resulting 927 blocks consist of only consecutive words.\n",
    "\n",
    "Moreover, in Hebrew writing, when a word has an article that has and a prefixed preposition, the article is elided. Therefore, it is no longer visible, except in vocalised texts (such as the 10th-century Masoretic Text). As BHSA is based on an edition of the Masoretic Text (the BHS), it includes the information about 'hidden' articles. As the goal of this research is to predict phrase atom boundaries for the Dead Sea Scrolls - which are unvocalised texts - this added information is ignored and deleted. In the dataset of the BHSA, these words have an empty string ('') as the value for the feature g_cons, the transliterated consonantal presentation of words. \n",
    "\n",
    "Also, when a word has a pronominal suffix, in the pre-processing phase, this suffix will be separated from that word and considered as a word on its own. This way, the pronominal suffix becomes similar to the 'normal' personal pronouns. More importantly, regarding the pronominal suffix as a separate, individual word matches the encoding of the DSS, the target text set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_hebrew_blocks():\n",
    "    \n",
    "    hebrew_blocks = collections.defaultdict(list)\n",
    "    chapters = [chap for chap in F.otype.s(\"chapter\")]\n",
    "    \n",
    "    block_index = 0\n",
    "    # iterates over all chapters\n",
    "    for chap in chapters:\n",
    "        chap_words = []\n",
    "        \n",
    "        # iterates over and collects all words except the elided-he\n",
    "        # adds an extra word if there is a pronominal suffix\n",
    "        for word in L.d(chap, \"word\"):\n",
    "            if F.g_cons.v(word) != '':\n",
    "                chap_words.append(word)\n",
    "                if F.prs.v(word) not in ['absent', 'n/a']:\n",
    "                    chap_words.append(word)\n",
    "        \n",
    "        # splits chapter into blocks when it encounters non-Hebrew words \n",
    "        for node in range(len(chap_words)):\n",
    "            if F.language.v(chap_words[node]) == 'Hebrew':\n",
    "                hebrew_blocks[block_index].append(chap_words[node])\n",
    "            elif F.language.v(chap_words[node]) != 'Hebrew':\n",
    "                if F.language.v(chap_words[node - 1]) == 'Hebrew':\n",
    "                    block_index += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "        block_index += 1\n",
    "    \n",
    "    \n",
    "    # shuffles the blocks randomly\n",
    "    indexes = shuffle(list(hebrew_blocks.keys()))\n",
    "    hebrew_blocks = {k: hebrew_blocks[k] for k in indexes}\n",
    "\n",
    "    return hebrew_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an example of what the 'hebrew blocks' look like, here are the first ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daniel 10 1-21',\n",
       " 'Judges 5 1-31',\n",
       " 'Nahum 2 1-14',\n",
       " 'Judges 4 1-24',\n",
       " 'Psalms 110 1-7',\n",
       " '2 Samuel 22 1-51',\n",
       " 'Judges 6 1-40',\n",
       " '1 Kings 18 1-46',\n",
       " 'Psalms 136 1-26',\n",
       " 'Micah 1 1-16']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hebrew_blocks = create_hebrew_blocks()\n",
    "[\" \".join([str(i) for i in T.sectionFromNode(words[0])]).replace(\"_\", \" \") + \"-\" + str(T.sectionFromNode(words[-1])[2]) for words in hebrew_blocks.values()][:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is defined, the next step is to collect the input and output data. For these purposes, the following three functions are used. The first function, *get_pos*, returns the part of speech when the input is a word and extends the part of speech - if needed - by the word's state. The second function returns a 'p' when the word is the end of a phrase atom, and an 'x' when it is not. The third function iterates through each block and all words and adds the input and output to each word. The resulting blocks, that now also contain input and output data, are split into training blocks and test blocks according to a predefined ratio of 9:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(w):\n",
    "    # customises the part of speech for a word and returns it\n",
    "    \n",
    "    # when a word has a suffix and a defined state, \n",
    "    # its part of speech is extended by '_c' indicating a construct state.\n",
    "    if F.prs.v(w) not in ['absent', 'n/a'] and F.st.v(w) != \"NA\":\n",
    "        pos = str(F.sp.v(w)) + \"_c\"\n",
    "        \n",
    "    # in all other cases, when a word has a state and no suffix, the \n",
    "    # part of speech is extended by the state\n",
    "    elif F.st.v(w) != \"NA\":\n",
    "        pos = str(F.sp.v(w)) + \"_\" + str(F.st.v(w))\n",
    "        \n",
    "    # when the word has neither state nor suffix, its part of speech remains unchanged\n",
    "    else:\n",
    "        pos = str(F.sp.v(w))\n",
    "\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_in_phrase_atom(w):\n",
    "    # returns an 'p\" when a word is the end of a phrase atom and an 'x' if it is not.\n",
    "\n",
    "    ph_atom = L.u(w, 'phrase_atom')[0]\n",
    "    words_in_ph_atom = L.d(ph_atom, \"word\")\n",
    "    \n",
    "    # when the word is the end of the phrase atom\n",
    "    if w == words_in_ph_atom[-1]:\n",
    "        ph_atom_end = 'p'\n",
    "    \n",
    "    # when it is not\n",
    "    else:\n",
    "        ph_atom_end = \"x\"\n",
    "\n",
    "    return ph_atom_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(hebrew_blocks, ratio=0.9):\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # iterates through all blocks\n",
    "    for block_idx, block_words in hebrew_blocks.items():\n",
    "        block_data = []\n",
    "        done = False\n",
    "\n",
    "        # iterates through all words\n",
    "        for w in block_words:\n",
    "            \n",
    "            # looks up the phrase to find the phrase function later\n",
    "            phrase = L.u(w, \"phrase\")[0]\n",
    "\n",
    "            # checks whether a word has a suffix\n",
    "            if done == True:\n",
    "                done = False\n",
    "                continue\n",
    "\n",
    "            # when a word appears twice in a block, the second one represents the suffix\n",
    "            # the following lines make sure that the suffix gets a fitting part of speech\n",
    "            # (prps) and phrase atom position\n",
    "\n",
    "            elif block_words.count(w) == 2:\n",
    "                # if the word has a suffix the data collection will happen for both the\n",
    "                # word and the suffix. The second time the word passes the loop, it is ignored\n",
    "                # by setting the bolean 'done' to true.\n",
    "                done = True\n",
    "\n",
    "                # if the phrase function of the word indicates a SUBJECT or OBJECT suffix\n",
    "                if F.function.v(phrase)[-1] in \"SO\":\n",
    "                    \n",
    "                    # if it is the end of a phrase atom, the suffix becomes a separte phrase atom\n",
    "                    if position_in_phrase_atom(w) == 'p':\n",
    "                        block_data.append(['p', get_pos(w), w])\n",
    "                        block_data.append(['p', 'prps', w])\n",
    "                    \n",
    "                    # if it is not, both original word and suffix remain 'x' for the same phrase atom\n",
    "                    else:\n",
    "                        block_data.append(['x', get_pos(w), w])\n",
    "                        block_data.append(['x', 'prps', w])\n",
    "\n",
    "                # if the phrase function does not indicate a subject or object suffix\n",
    "                # the suffix takes over the phrase atom position form its base word.\n",
    "                # If it becomes the end of phrase atom because of this, the base word gets an 'x'\n",
    "                else:\n",
    "                    if position_in_phrase_atom(w) == 'p':\n",
    "                        block_data.append(['x', get_pos(w), w])\n",
    "                        block_data.append(['p', 'prps', w])\n",
    "                    else:\n",
    "                        block_data.append(['x', get_pos(w), w])\n",
    "                        block_data.append(['x', 'prps', w])\n",
    "\n",
    "            # in all other cases, without suffixes involved, the phrase atom position and part of speech\n",
    "            # are determined in the regular way\n",
    "            else:\n",
    "                block_data.append([position_in_phrase_atom(w), get_pos(w), w])\n",
    "        data[block_idx] = block_data\n",
    "    \n",
    "    # shuffles the data randomly by block index\n",
    "    data = {k: data[k] for k in shuffle(list(data.keys()))}\n",
    "    \n",
    "    # splits the shuffled data into train blocks and test blocks according to the preset ratio\n",
    "    keys = list(data.keys())\n",
    "    train_blocks = {k: data[k] for k in keys[:int(len(keys) * ratio)]}\n",
    "    test_blocks = {k: data[k] for k in keys[int(len(keys) * ratio):]}\n",
    "\n",
    "    return train_blocks, test_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the data of the first ten words of the test blocks looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['p', 'verb', 226065],\n",
       " ['x', 'prep', 226066],\n",
       " ['p', 'prps', 226066],\n",
       " ['p', 'subs_a', 226067],\n",
       " ['p', 'conj', 226068],\n",
       " ['p', 'subs_a', 226069],\n",
       " ['p', 'verb', 226070],\n",
       " ['p', 'subs_a', 226071],\n",
       " ['p', 'verb', 226072],\n",
       " ['p', 'advb', 226073]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blocks, test_blocks = collect_data(hebrew_blocks)\n",
    "[words for words in train_blocks.values()][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting train and test blocks consist of blocks containing the following three features for each word:\n",
    "1. The part of speech of that word\n",
    "2. The corresponding value for its position in a phrase atom (an 'x' or a 'p')\n",
    "3. The word's node, which is an integer that is unique for each word and connects the word to the structure of the database. This helps to evaluate the results on a word level. \n",
    "\n",
    "The following two functions create the input and output sequences for the train and test set. In addition to this, each unique input and output value for every single word is collected in the input and output vocabularies. Lastly, the maximum length of the input and output sequences is calculated. These parameters are useful for choosing the dimensions of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_data(train_blocks):\n",
    "    ip_pos_seq = []\n",
    "    op_ph_seq = []\n",
    "    ip_pos_voc = set()\n",
    "    op_ph_voc = set()\n",
    "    \n",
    "    # iterates over all training blocks\n",
    "    for train_word_nodes in train_blocks.values():\n",
    "        \n",
    "        # iterates over all words except the last 8, \n",
    "        # this way the last sequence won't run out of words\n",
    "        # and have exactly 9 words \n",
    "        for w in range(len(train_word_nodes[:-8])):\n",
    "            \n",
    "            # the following lines collect the training data \n",
    "            # for 9 consecutive words in a list\n",
    "            \n",
    "            # input data: part of speech\n",
    "            pos = [train_word_nodes[w][1] for w in range(w, w + 9)]\n",
    "            \n",
    "            # output data: position in phrase atom\n",
    "            ph_atom = [\n",
    "                train_word_nodes[w][0] for w in range(w, w + 9)\n",
    "            ]\n",
    "            \n",
    "            # adds the start and stop symbol\n",
    "            ph_atom = ['\\t'] + ph_atom + ['\\n']\n",
    "            \n",
    "            # collects the input and output for this word (w)\n",
    "            # in a list\n",
    "            ip_pos_seq.append(pos)\n",
    "            op_ph_seq.append(ph_atom)\n",
    "            \n",
    "            # collects all unique input and output values in vocabularies\n",
    "            for p in pos:\n",
    "                ip_pos_voc.add(p)\n",
    "            for ph in ph_atom:\n",
    "                op_ph_voc.add(ph)\n",
    "                \n",
    "    # sorts the vocabuluries and converts them into lists\n",
    "    ip_pos_voc = sorted(list(ip_pos_voc))\n",
    "    op_ph_voc = sorted(list(op_ph_voc))\n",
    "    \n",
    "    # calculated the the maximum lenght of input and output sequences\n",
    "    max_len_ip = max([len(pos) for pos in ip_pos_seq])\n",
    "    max_len_op = max([len(ph) for ph in op_ph_seq])\n",
    "    \n",
    "    # shuffles all sequences randomly\n",
    "    ip_pos_seq, op_ph_seq = shuffle(ip_pos_seq, op_ph_seq)\n",
    "\n",
    "    return ip_pos_seq, op_ph_seq, ip_pos_voc, op_ph_voc, max_len_ip, max_len_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the first four input and output sequences look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nmpr_a', 'conj', 'verb', 'art', 'adjv_a', 'conj', 'art', 'adjv_a', 'conj']\n",
      "['\\t', 'p', 'p', 'p', 'x', 'x', 'x', 'x', 'x', 'x', '\\n']\n",
      "['subs_c', 'prps', 'prde', 'subs_c', 'subs_c', 'nmpr_a', 'prep', 'subs_c', 'prps']\n",
      "['\\t', 'x', 'p', 'p', 'x', 'x', 'p', 'x', 'x', 'p', '\\n']\n",
      "['prps', 'prep', 'nmpr_a', 'subs_c', 'prps', 'nega', 'verb', 'subs_a', 'prep']\n",
      "['\\t', 'p', 'x', 'p', 'x', 'p', 'p', 'p', 'p', 'x', '\\n']\n",
      "['prep', 'prps', 'prep', 'nmpr_a', 'subs_c', 'prps', 'conj', 'verb', 'prps']\n",
      "['\\t', 'x', 'p', 'x', 'p', 'x', 'p', 'p', 'p', 'p', '\\n']\n"
     ]
    }
   ],
   "source": [
    "ip_pos_seq, op_ph_seq = prep_train_data(train_blocks)[:2]\n",
    "for i in range(0, 4):\n",
    "    print(ip_pos_seq[i])    \n",
    "    print(op_ph_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_test_data(test_blocks):\n",
    "    \n",
    "    ip_pos_test = {}\n",
    "    op_ph_test = {}\n",
    "    \n",
    "    for block_idx, test_word_nodes in test_blocks.items():\n",
    "        \n",
    "        ip_pos_test_block = []\n",
    "        op_ph_test_block = []\n",
    "        \n",
    "        for w in range(len(test_word_nodes[:-8])):\n",
    "            \n",
    "            # collects test data\n",
    "            pos = [test_word_nodes[w][1] for w in range(w, w + 9)]\n",
    "            ph_atom = [test_word_nodes[w][0] for w in range(w, w + 9)]\n",
    "            \n",
    "            ip_pos_test_block.append(pos)\n",
    "            op_ph_test_block.append(ph_atom)\n",
    "            \n",
    "        ip_pos_test[block_idx] = ip_pos_test_block\n",
    "        op_ph_test[block_idx] = op_ph_test_block\n",
    "\n",
    "    return ip_pos_test, op_ph_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then transformed because the neural network can only handle numerical data. To convert the numeric data back to the original data, the following dictionaries are created to map the input and output vocabularies to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(ip_pos_voc, op_ph_voc):\n",
    "    \n",
    "    # maps the input vocabulary of part of speech to indeces\n",
    "    ip_idx2pos = {}\n",
    "    ip_pos2idx = {}\n",
    "\n",
    "    for k, v in enumerate(ip_pos_voc):\n",
    "        ip_idx2pos[k] = v\n",
    "        ip_pos2idx[v] = k\n",
    "    \n",
    "    # maps the output vocabulary of phrase atom position to indeces\n",
    "    op_idx2ph = {}\n",
    "    op_ph2idx = {}\n",
    "\n",
    "    for k, v in enumerate(op_ph_voc):\n",
    "        op_idx2ph[k] = v\n",
    "        op_ph2idx[v] = k\n",
    "\n",
    "    return ip_idx2pos, ip_pos2idx, op_idx2ph, op_ph2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the input and output data are categorical, the data is being one-hot encoded. This means that each input value is represented by an array containing as many values as there are values in the input variable. The arrays contain zero's, with a 1 on the place of the integer value of the input.\n",
    "An input for a single word might look like \n",
    "          \n",
    "            [1, 0, 0, ... , 0, 0] \n",
    "which corresponds with the integer value 1 of the input vocabulary, which is 'adjv_a', an adjective with an absolute state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(max_len_ip, max_len_op, ip_pos_voc, op_ph_voc, ip_pos2idx,\n",
    "                   op_ph2idx, ip_pos_test, op_ph_seq):\n",
    "    \n",
    "    # creates three-dimensional numpy arrays\n",
    "    one_hot_ip = np.zeros(shape=(len(ip_pos_test), max_len_ip, len(ip_pos_voc)),\n",
    "                      dtype='float32')\n",
    "    one_hot_op = np.zeros(shape=(len(ip_pos_test), max_len_op, len(op_ph_voc)),\n",
    "                      dtype='float32')\n",
    "    target_data = np.zeros((len(ip_pos_test), max_len_op, len(op_ph_voc)),\n",
    "                           dtype='float32')\n",
    "\n",
    "    for i in range(len(ip_pos_test)):\n",
    "        for k, ps in enumerate(ip_pos_test[i]):\n",
    "            one_hot_ip[i, k, ip_pos2idx[ps]] = 1\n",
    "\n",
    "        for k, ph in enumerate(op_ph_seq[i]):\n",
    "            one_hot_op[i, k, op_ph2idx[ph]] = 1\n",
    "            \n",
    "            # the decoder target data is ahead one timestep and does \n",
    "            # not include the start symbol\n",
    "            if k > 0:\n",
    "                target_data[i, k - 1, op_ph2idx[ph]] = 1\n",
    "\n",
    "    return one_hot_ip, one_hot_op, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates the structure of the neural network, which has an encoder-decoder architecture. The encoder consists of an *input layer* that has as many cells as the size of the input vocabulary of parts of speech and two *LSTM layers* which both have 250 cells. The *input layer* of the decoder has as many cells as the size of the output vocabulary of x's and p's. The decoder also has a *LSTM layer* of 250 cells, and a *dense layer* of exactly as many cells as the output vocabulary. The dense layer uses the softmax activation to normalise the outputs into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(ip_pos_voc, op_ph_voc):\n",
    "    \n",
    "    # encoder model\n",
    "    encoder_input = Input(shape=(None, len(ip_pos_voc)))\n",
    "    encoder_LSTM = LSTM(250,\n",
    "                        activation='relu',\n",
    "                        return_state=True,\n",
    "                        return_sequences=True)(encoder_input)\n",
    "    encoder_LSTM = LSTM(250, return_state=True)(encoder_LSTM)\n",
    "    encoder_outputs, encoder_h, encoder_c = encoder_LSTM\n",
    "    encoder_states = [encoder_h, encoder_c]\n",
    "    \n",
    "    # decoder model\n",
    "    decoder_input = Input(shape=(None, len(op_ph_voc)))\n",
    "    decoder_LSTM = LSTM(250, return_sequences=True, return_state=True)\n",
    "    decoder_out, _, _ = decoder_LSTM(decoder_input,\n",
    "                                     initial_state=encoder_states)\n",
    "    decoder_dense = Dense(len(op_ph_voc), activation='softmax')\n",
    "    decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return encoder_input, encoder_states, decoder_input, decoder_LSTM, decoder_dense, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's architecture is defined and the next step is to feed the data into the model. First, stopping conditions are defined. When these are met, the model is finished and stops running. Then, the optimiser and loss function are set. Finally, the model is fed the training data and begins fitting itself to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, one_hot_ip, one_hot_op, target_data, batch_size,\n",
    "                      epochs, val_split):\n",
    "    # defines stop conditions\n",
    "    callback = EarlyStopping(monitor='val_loss',\n",
    "                             patience=patience,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "    \n",
    "    # defines optimizer\n",
    "    adam = Adam(lr=0.0008, beta_1=0.99, beta_2=0.999, epsilon=0.00000001)\n",
    "    \n",
    "    # compiles the model\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # fits the model to the training data\n",
    "    model.fit(x=[one_hot_ip, one_hot_op],\n",
    "              y=target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=val_split,\n",
    "              callbacks=[callback])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script sets all parameters and then runs all functions mentioned above. The data is collected, created, pre-processed and the network is defined and compiled. In the end, the model is fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 18)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 250),  269000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 250), (None, 501000      lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 250),  255000      input_2[0][0]                    \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 4)      1004        lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,026,004\n",
      "Trainable params: 1,026,004\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 385840 samples, validate on 20308 samples\n",
      "Epoch 1/150\n",
      "385840/385840 [==============================] - 20s 52us/step - loss: 0.3154 - accuracy: 0.8170 - val_loss: 0.2831 - val_accuracy: 0.8295\n",
      "Epoch 2/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.2408 - accuracy: 0.8665 - val_loss: 0.2051 - val_accuracy: 0.8944\n",
      "Epoch 3/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.1641 - accuracy: 0.9204 - val_loss: 0.1313 - val_accuracy: 0.9367\n",
      "Epoch 4/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.1078 - accuracy: 0.9478 - val_loss: 0.0913 - val_accuracy: 0.9547\n",
      "Epoch 5/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0856 - accuracy: 0.9572 - val_loss: 0.0823 - val_accuracy: 0.9591\n",
      "Epoch 6/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0774 - accuracy: 0.9734 - val_loss: 0.0735 - val_accuracy: 0.9821\n",
      "Epoch 7/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0715 - accuracy: 0.9828 - val_loss: 0.0697 - val_accuracy: 0.9825\n",
      "Epoch 8/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0677 - accuracy: 0.9836 - val_loss: 0.0680 - val_accuracy: 0.9835\n",
      "Epoch 9/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0658 - accuracy: 0.9843 - val_loss: 0.0661 - val_accuracy: 0.9840\n",
      "Epoch 10/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0643 - accuracy: 0.9848 - val_loss: 0.0649 - val_accuracy: 0.9846\n",
      "Epoch 11/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0630 - accuracy: 0.9853 - val_loss: 0.0628 - val_accuracy: 0.9854\n",
      "Epoch 12/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0621 - accuracy: 0.9856 - val_loss: 0.0626 - val_accuracy: 0.9853\n",
      "Epoch 13/150\n",
      "385840/385840 [==============================] - 19s 50us/step - loss: 0.0615 - accuracy: 0.9858 - val_loss: 0.0629 - val_accuracy: 0.9855\n",
      "Epoch 14/150\n",
      "385840/385840 [==============================] - 20s 53us/step - loss: 0.0608 - accuracy: 0.9861 - val_loss: 0.0614 - val_accuracy: 0.9859\n",
      "Epoch 15/150\n",
      "385840/385840 [==============================] - 20s 51us/step - loss: 0.0601 - accuracy: 0.9864 - val_loss: 0.0612 - val_accuracy: 0.9860\n",
      "Epoch 16/150\n",
      "385840/385840 [==============================] - 21s 54us/step - loss: 0.0594 - accuracy: 0.9866 - val_loss: 0.0604 - val_accuracy: 0.9861\n",
      "Epoch 17/150\n",
      "385840/385840 [==============================] - 21s 55us/step - loss: 0.0590 - accuracy: 0.9868 - val_loss: 0.0598 - val_accuracy: 0.9865\n",
      "Epoch 18/150\n",
      "385840/385840 [==============================] - 21s 56us/step - loss: 0.0588 - accuracy: 0.9869 - val_loss: 0.0600 - val_accuracy: 0.9865\n",
      "Epoch 19/150\n",
      "385840/385840 [==============================] - 20s 53us/step - loss: 0.0583 - accuracy: 0.9871 - val_loss: 0.0589 - val_accuracy: 0.9868\n",
      "Epoch 20/150\n",
      "385840/385840 [==============================] - 19s 48us/step - loss: 0.0577 - accuracy: 0.9873 - val_loss: 0.0589 - val_accuracy: 0.9868\n",
      "Epoch 21/150\n",
      "385840/385840 [==============================] - 19s 48us/step - loss: 0.0576 - accuracy: 0.9873 - val_loss: 0.0591 - val_accuracy: 0.9868\n",
      "Epoch 22/150\n",
      "385840/385840 [==============================] - 19s 48us/step - loss: 0.0572 - accuracy: 0.9875 - val_loss: 0.0593 - val_accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "epochs = 150\n",
    "val_split = 0.05\n",
    "patience = 3\n",
    "ratio = 0.9\n",
    "\n",
    "# collects the relevant parts of the Hebrew Bible\n",
    "hebrew_blocks = create_hebrew_blocks()\n",
    "\n",
    "# collects input and output data and creates training and test sets\n",
    "train_blocks, test_blocks = collect_data(hebrew_blocks, ratio)\n",
    "\n",
    "# creates training sequences\n",
    "ip_pos_seq, op_ph_seq, ip_pos_voc, op_ph_voc, max_len_ip, max_len_op = prep_train_data(\n",
    "    train_blocks)\n",
    "\n",
    "# creates test sequences\n",
    "ip_pos_test, op_ph_test = prep_test_data(test_blocks)\n",
    "\n",
    "# converts data to numerical data\n",
    "ip_idx2pos, ip_pos2idx, op_idx2ph, op_ph2idx = create_dicts(\n",
    "    ip_pos_voc, op_ph_voc)\n",
    "\n",
    "# one-hot encodes the data\n",
    "one_hot_ip, one_hot_op, target_data = one_hot_encode(max_len_ip, max_len_op,\n",
    "                                                     ip_pos_voc, op_ph_voc,\n",
    "                                                     ip_pos2idx, op_ph2idx,\n",
    "                                                     ip_pos_seq, op_ph_seq)\n",
    "\n",
    "one_hot_test_data = {\n",
    "    block:\n",
    "    one_hot_encode(max_len_ip, max_len_op, ip_pos_voc, op_ph_voc, ip_pos2idx,\n",
    "                   op_ph2idx, ip_pos_test[block], op_ph_seq)[0]\n",
    "    for block in test_blocks\n",
    "}\n",
    "\n",
    "# defines the model\n",
    "encoder_input, encoder_states, decoder_input, decoder_LSTM, decoder_dense, model = define_LSTM_model(\n",
    "    ip_pos_voc, op_ph_voc)\n",
    "\n",
    "# fits the model to the training data\n",
    "model = compile_and_train(model, one_hot_ip, one_hot_op, target_data,\n",
    "                          batch_size, epochs, val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 22 epochs, the stopping conditions were met and the model stopped training. It reached an accuracy of 98.67% on the validation set (5% of the training data that was set aside for self-evaluation). Although this is a decent result, it is more important to find out how accurate the model is on completely new data. This is where the test set, the 10% that was set apart at the beginning, comes in.\n",
    "\n",
    "First, a few more functions are needed to be able to convert input data into predicted outcomes. The function *prediction_dict* converts the predicted sequences of 9 words into phrase atom boundary predictions for each individual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_model(encoder_input, encoder_states, decoder_LSTM, decoder_dense):\n",
    "    # encoder inference model\n",
    "    encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "    # decoder inference model\n",
    "    decoder_state_input_h = Input(shape=(250, ))\n",
    "    decoder_state_input_c = Input(shape=(250, ))\n",
    "    decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_out, decoder_h, decoder_c = decoder_LSTM(\n",
    "        decoder_input, initial_state=decoder_input_states)\n",
    "\n",
    "    decoder_states = [decoder_h, decoder_c]\n",
    "\n",
    "    decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "    decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                              outputs=[decoder_out] + decoder_states)\n",
    "\n",
    "    return encoder_model_inf, decoder_model_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *decode_seq()* uses the trained model to predict output sequences. It takes one-hot encoded sequences of words as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(ip_seq, encoder_model_inf, decoder_model_inf, op_ph_voc,\n",
    "               op_ph2idx, op_idx2ph):\n",
    "\n",
    "    states_val = encoder_model_inf.predict(ip_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, len(op_ph_voc)))\n",
    "    target_seq[0, 0, op_ph2idx['\\t']] = 1\n",
    "\n",
    "    pred_ph = []\n",
    "    stop_condition = False\n",
    "\n",
    "    while not stop_condition:\n",
    "\n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(\n",
    "            x=[target_seq] + states_val)\n",
    "\n",
    "        max_val_index = np.argmax(decoder_out[0, -1, :])\n",
    "        sampled_out_char = op_idx2ph[max_val_index]\n",
    "        pred_ph.append(sampled_out_char)\n",
    "\n",
    "        if (sampled_out_char == '\\n'):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, len(op_ph_voc)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "\n",
    "        states_val = [decoder_h, decoder_c]\n",
    "\n",
    "    return pred_ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *prediction_dict()* converts the predicted outputs for sequences into predicted outputs for single words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_dict(test_blocks, one_hot_test_data, op_ph_test):\n",
    "    decision_dict = {}\n",
    "    for block, block_seqs in test_blocks.items():\n",
    "        \n",
    "        decision_dict_block = collections.defaultdict(list)\n",
    "        \n",
    "        for seq_index in range(len(one_hot_test_data[block])):\n",
    "            ip_seq = one_hot_test_data[block][seq_index:seq_index+1]\n",
    "            \n",
    "            pred_ph = decode_seq(ip_seq, encoder_model_inf, decoder_model_inf, op_ph_voc,\n",
    "               op_ph2idx, op_idx2ph)\n",
    "            if len(pred_ph[:-1]) == len(op_ph_test[block][seq_index]):\n",
    "                for pred_index in range(len(pred_ph[:-1])):\n",
    "                    decision_dict_block[seq_index + pred_index].append(pred_ph[:-1][pred_index])\n",
    "        decision_dict[block] = decision_dict_block\n",
    "    \n",
    "    return decision_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *safe_div()* divides two numbers and returns the result. If the denominator is zero, it returns zero. This function comes in handy when calculating percentages in the evaluation later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(numerator, denominator):\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function runs all words in the test set through the model and counts the correct and false predictions. Of the latter, it also registers the corresponding part of speech to get insight into the performance of the model per input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(test_blocks, decision_dict):\n",
    "    correct_test = 0\n",
    "    wrong_test = 0\n",
    "    bible_section = []\n",
    "    pos_dict = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "    cross_dict = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "    \n",
    "    # iterates through all blocks\n",
    "    for block in test_blocks:\n",
    "        \n",
    "        # iterates through all words\n",
    "        for key in range(len(test_blocks[block])):\n",
    "            w = test_blocks[block][key][2]\n",
    "            \n",
    "            # collects all predictions for the word (up to 9)\n",
    "            data = collections.Counter(decision_dict[block][key])\n",
    "            \n",
    "            # determines the most common prediction\n",
    "            pred = data.most_common(1)[0][0]\n",
    "            \n",
    "            # counts each possible combination of true and predicted output\n",
    "            cross_dict[test_blocks[block][key][0]][pred] += 1\n",
    "            \n",
    "            # if the prediction is correct\n",
    "            if test_blocks[block][key][0] == pred:\n",
    "                correct_test += 1\n",
    "            \n",
    "            # if the prediciton is false\n",
    "            else:\n",
    "                wrong_test += 1\n",
    "                \n",
    "                # registers the exact location in the BHSA of the misprediction\n",
    "                # along with information about the output\n",
    "                bible_section.append(\n",
    "                    str(w) + \" \" + T.sectionFromNode(w)[0].replace(\"_\", \" \") +\n",
    "                    \" \" + str(T.sectionFromNode(w)[1]) + \":\" +\n",
    "                    str(T.sectionFromNode(w)[2]) + \" \" +\n",
    "                    test_blocks[block][key][1] + \" \" +\n",
    "                    test_blocks[block][key][0] + \" \" +\n",
    "                    data.most_common(1)[0][0])\n",
    "                \n",
    "                # registers input corresponding with the misprediciton\n",
    "                pos = test_blocks[block][key][1]\n",
    "                pos_dict[pos][pred] = pos_dict[pos].get(pred, 0) + 1\n",
    "                \n",
    "    # creates an extensive evaluation of errors by part of speech\n",
    "    eval_by_pos = {}\n",
    "    for k in pos_dict.keys():\n",
    "        total_pos = len([\n",
    "            test_blocks[block][key][2] for block in test_blocks\n",
    "            for key in range(len(test_blocks[block]))\n",
    "            if test_blocks[block][key][1] == k\n",
    "        ])\n",
    "        total_pos_ph_x = len([\n",
    "            test_blocks[block][key][2] for block in test_blocks\n",
    "            for key in range(len(test_blocks[block]))\n",
    "            if test_blocks[block][key][1] == k\n",
    "            and test_blocks[block][key][0] == 'x'\n",
    "        ])\n",
    "        total_pos_ph_p = len([\n",
    "            test_blocks[block][key][2] for block in test_blocks\n",
    "            for key in range(len(test_blocks[block]))\n",
    "            if test_blocks[block][key][1] == k\n",
    "            and test_blocks[block][key][0] == 'p'\n",
    "        ])\n",
    "        total_wrong = pos_dict[k]['x'] + pos_dict[k]['p']\n",
    "\n",
    "        pct_x = 100 * safe_div(pos_dict[k]['p'], total_pos_ph_x)\n",
    "        pct_p = 100 * safe_div(pos_dict[k]['x'], total_pos_ph_p)\n",
    "        pct_tot = 100 * \\\n",
    "            safe_div(total_wrong, total_pos)\n",
    "\n",
    "        eval_by_pos[k] = {\n",
    "            \"Total in Test Set\": total_pos,\n",
    "            \"Total Mistakes\": total_wrong,\n",
    "            \"Mistakes Percentage\": pct_tot,\n",
    "            \"Total 'x' in Test Set\": total_pos_ph_x,\n",
    "            \"Mistaken for '\" + 'p' + \"'\": pos_dict[k]['p'],\n",
    "            \"Percentage 'x'\": pct_x,\n",
    "            \"Total '\" + 'p' + \"' in Test Set\": total_pos_ph_p,\n",
    "            \"Mistaken for 'x'\": pos_dict[k]['x'],\n",
    "            \"Percentage '\" + 'p' + \"'\": pct_p\n",
    "        }\n",
    "\n",
    "    eval_by_pos = {\n",
    "        item[0]: item[1]\n",
    "        for item in sorted(eval_by_pos.items(),\n",
    "                           key=lambda x: (x[1][\"Total Mistakes\"]),\n",
    "                           reverse=True)\n",
    "    }\n",
    "\n",
    "    df_eval_by_pos = pd.DataFrame.from_dict(eval_by_pos).T\n",
    "    int_cols = [\n",
    "        \"Total in Test Set\", \"Total Mistakes\", \"Total 'x' in Test Set\", \"Mistaken for 'x'\",\n",
    "        \"Total '\" + 'p' + \"' in Test Set\", \"Mistaken for '\" + 'p' + \"'\"\n",
    "    ]\n",
    "    float_cols = [\n",
    "        \"Mistakes Percentage\", \"Percentage 'x'\", \"Percentage '\" + 'p' + \"'\"\n",
    "    ]\n",
    "    \n",
    "    # creates a data frame containing the evaluation per the part of speech \n",
    "    df_eval_by_pos[int_cols] = df_eval_by_pos[int_cols].applymap(np.int64)\n",
    "    df_eval_by_pos[float_cols] = df_eval_by_pos[float_cols].round(2)\n",
    "\n",
    "    # creates a cross evaluation\n",
    "    cross_eval = [[\n",
    "        cross_dict[key][key2] if key2 in cross_dict[key] else 0\n",
    "        for key2 in list(cross_dict.keys())\n",
    "    ] for key in list(cross_dict.keys())]\n",
    "    df_cross_eval = pd.DataFrame(\n",
    "        cross_eval,\n",
    "        columns=[\"End of Phrase Atom\", \"Not \" + \"End of Phrase Atom\"],\n",
    "        index=[\"Predicted as End\", \"Predicted as Not End\"])\n",
    "\n",
    "    eval_summary = {\n",
    "        \"Correct Classifications\":\n",
    "        correct_test,\n",
    "        \"Misclassifications\":\n",
    "        wrong_test,\n",
    "        \"Accuracy\":\n",
    "        round(100 * safe_div(correct_test, (correct_test + wrong_test)), 2)\n",
    "    }\n",
    "    print(\"Accuracy:\",\n",
    "          round(100 * safe_div(correct_test, (correct_test + wrong_test)), 2))\n",
    "    \n",
    "    # creates a dataframe of the cross evaluation\n",
    "    df_eval_summary = pd.DataFrame(eval_summary, index=[\"Value\"])\n",
    "\n",
    "    return df_eval_by_pos, df_cross_eval, df_eval_summary, bible_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script runs the previous functions to predict the outcomes for the test set, does some evaluations, and displays the results in tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.46\n"
     ]
    }
   ],
   "source": [
    "# creates the encoder and decoder inference model\n",
    "encoder_model_inf, decoder_model_inf = encoder_decoder_model(\n",
    "    encoder_input, encoder_states, decoder_LSTM, decoder_dense)\n",
    "\n",
    "# creates the decision dictionary containing up to predicted outcomes for each word\n",
    "decision_dict = prediction_dict(test_blocks, one_hot_test_data, op_ph_test)\n",
    "\n",
    "# evaluates the results and publishes the results in tables\n",
    "df_eval_by_pos, df_cross_eval, df_eval_summary, bible_section = test_evaluation(\n",
    "    test_blocks, decision_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct Classifications</th>\n",
       "      <th>Misclassifications</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>44139</td>\n",
       "      <td>1620</td>\n",
       "      <td>96.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Correct Classifications  Misclassifications  Accuracy\n",
       "Value                    44139                1620     96.46"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End of Phrase Atom</th>\n",
       "      <th>Not End of Phrase Atom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted as End</th>\n",
       "      <td>26725</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted as Not End</th>\n",
       "      <td>1121</td>\n",
       "      <td>17414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      End of Phrase Atom  Not End of Phrase Atom\n",
       "Predicted as End                   26725                     499\n",
       "Predicted as Not End                1121                   17414"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to predict the phrase atom boundaries for the test set correctly for 96.46% of the words. It is important to analyse the model's performance further. Therefore, the results are evaluated more specifically. The following table shows the errors per part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total in Test Set</th>\n",
       "      <th>Total Mistakes</th>\n",
       "      <th>Mistakes Percentage</th>\n",
       "      <th>Total 'x' in Test Set</th>\n",
       "      <th>Mistaken for 'p'</th>\n",
       "      <th>Percentage 'x'</th>\n",
       "      <th>Total 'p' in Test Set</th>\n",
       "      <th>Mistaken for 'x'</th>\n",
       "      <th>Percentage 'p'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subs_a</th>\n",
       "      <td>6083</td>\n",
       "      <td>493</td>\n",
       "      <td>8.10</td>\n",
       "      <td>1251</td>\n",
       "      <td>349</td>\n",
       "      <td>27.90</td>\n",
       "      <td>4832</td>\n",
       "      <td>144</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conj</th>\n",
       "      <td>6051</td>\n",
       "      <td>416</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1074</td>\n",
       "      <td>293</td>\n",
       "      <td>27.28</td>\n",
       "      <td>4977</td>\n",
       "      <td>123</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmpr_a</th>\n",
       "      <td>2803</td>\n",
       "      <td>146</td>\n",
       "      <td>5.21</td>\n",
       "      <td>367</td>\n",
       "      <td>102</td>\n",
       "      <td>27.79</td>\n",
       "      <td>2436</td>\n",
       "      <td>44</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb_c</th>\n",
       "      <td>535</td>\n",
       "      <td>132</td>\n",
       "      <td>24.67</td>\n",
       "      <td>211</td>\n",
       "      <td>94</td>\n",
       "      <td>44.55</td>\n",
       "      <td>324</td>\n",
       "      <td>38</td>\n",
       "      <td>11.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prps</th>\n",
       "      <td>5097</td>\n",
       "      <td>110</td>\n",
       "      <td>2.16</td>\n",
       "      <td>200</td>\n",
       "      <td>96</td>\n",
       "      <td>48.00</td>\n",
       "      <td>4897</td>\n",
       "      <td>14</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subs_c</th>\n",
       "      <td>6084</td>\n",
       "      <td>98</td>\n",
       "      <td>1.61</td>\n",
       "      <td>5897</td>\n",
       "      <td>19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>187</td>\n",
       "      <td>79</td>\n",
       "      <td>42.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advb</th>\n",
       "      <td>451</td>\n",
       "      <td>71</td>\n",
       "      <td>15.74</td>\n",
       "      <td>97</td>\n",
       "      <td>60</td>\n",
       "      <td>61.86</td>\n",
       "      <td>354</td>\n",
       "      <td>11</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>2460</td>\n",
       "      <td>52</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2314</td>\n",
       "      <td>24</td>\n",
       "      <td>1.04</td>\n",
       "      <td>146</td>\n",
       "      <td>28</td>\n",
       "      <td>19.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjv_a</th>\n",
       "      <td>965</td>\n",
       "      <td>52</td>\n",
       "      <td>5.39</td>\n",
       "      <td>103</td>\n",
       "      <td>48</td>\n",
       "      <td>46.60</td>\n",
       "      <td>862</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb_a</th>\n",
       "      <td>1149</td>\n",
       "      <td>23</td>\n",
       "      <td>2.00</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>92.00</td>\n",
       "      <td>1124</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prep</th>\n",
       "      <td>6945</td>\n",
       "      <td>11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>6914</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>25.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nega</th>\n",
       "      <td>664</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>66.67</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prde</th>\n",
       "      <td>231</td>\n",
       "      <td>5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>50.00</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inrg</th>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>2.46</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>40.00</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intj</th>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.00</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjv_c</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total in Test Set  Total Mistakes  Mistakes Percentage  \\\n",
       "subs_a               6083             493                 8.10   \n",
       "conj                 6051             416                 6.87   \n",
       "nmpr_a               2803             146                 5.21   \n",
       "verb_c                535             132                24.67   \n",
       "prps                 5097             110                 2.16   \n",
       "subs_c               6084              98                 1.61   \n",
       "advb                  451              71                15.74   \n",
       "art                  2460              52                 2.11   \n",
       "adjv_a                965              52                 5.39   \n",
       "verb_a               1149              23                 2.00   \n",
       "prep                 6945              11                 0.16   \n",
       "nega                  664               5                 0.75   \n",
       "prde                  231               5                 2.16   \n",
       "inrg                  122               3                 2.46   \n",
       "intj                  201               2                 1.00   \n",
       "adjv_c                 62               1                 1.61   \n",
       "\n",
       "        Total 'x' in Test Set  Mistaken for 'p'  Percentage 'x'  \\\n",
       "subs_a                   1251               349           27.90   \n",
       "conj                     1074               293           27.28   \n",
       "nmpr_a                    367               102           27.79   \n",
       "verb_c                    211                94           44.55   \n",
       "prps                      200                96           48.00   \n",
       "subs_c                   5897                19            0.32   \n",
       "advb                       97                60           61.86   \n",
       "art                      2314                24            1.04   \n",
       "adjv_a                    103                48           46.60   \n",
       "verb_a                     25                23           92.00   \n",
       "prep                     6914                 3            0.04   \n",
       "nega                        3                 2           66.67   \n",
       "prde                        8                 4           50.00   \n",
       "inrg                        5                 2           40.00   \n",
       "intj                        2                 2          100.00   \n",
       "adjv_c                     60                 0            0.00   \n",
       "\n",
       "        Total 'p' in Test Set  Mistaken for 'x'  Percentage 'p'  \n",
       "subs_a                   4832               144            2.98  \n",
       "conj                     4977               123            2.47  \n",
       "nmpr_a                   2436                44            1.81  \n",
       "verb_c                    324                38           11.73  \n",
       "prps                     4897                14            0.29  \n",
       "subs_c                    187                79           42.25  \n",
       "advb                      354                11            3.11  \n",
       "art                       146                28           19.18  \n",
       "adjv_a                    862                 4            0.46  \n",
       "verb_a                   1124                 0            0.00  \n",
       "prep                       31                 8           25.81  \n",
       "nega                      661                 3            0.45  \n",
       "prde                      223                 1            0.45  \n",
       "inrg                      117                 1            0.85  \n",
       "intj                      199                 0            0.00  \n",
       "adjv_c                      2                 1           50.00  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_by_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the mistakes occurred in predicting the phrase atom position for words that were conjunctions or substantives with an absolute state (416 and 493 errors). Relatively, most errors occurred for verbs in the construct state and adverbs (24.67 and 15.74%). \n",
    "\n",
    "For a complete list of incorrect predictions, see the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal of this notebook was to predict phrase atom boundaries for the DSS package. For that reason, the model is tested on one scroll, namely, the Community Scroll (1QS). \n",
    "First, the extra-biblical package that contains this scroll is imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 8.3.0\n",
      "Api reference : https://annotation.github.io/text-fabric/cheatsheet.html\n",
      "\n",
      "72 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n",
      "  0.35s All features loaded/computed - for details use loadLog()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(locations='C:/Users/Mark/text-fabric-data/etcbc/extrabiblical/tf/0.2')\n",
    "\n",
    "api = TF.load('''\n",
    "    otype mother lex st typ code function rela det txt prs kind vs vt sp book chapter verse label language\n",
    "''')\n",
    "\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps of collecting and pre-processing the data are similar to the steps taken earlier when the model was trained on the BHSA. The main difference is that - this time - only one data set is created, which is the test set. As the model has already been trained, a training set is no longer needed. \n",
    "\n",
    "Moreover, there are some important differences between the structure of the data of the BHSA and the extra-biblical package:\n",
    "1. Most scrolls of the extra-biblical package are fragmentary, which means that some words are unreadable or missing from the scroll. Therefore, the data set does not always contain consecutive continuous words. To deal with this problem, the chapters are split into smaller blocks when and where ommissions occur (this process is similar to the splitting of the chapters of the BHSA when non-Hebrew words occurred).\n",
    "2. Contrary to the BHSA, in the extra-biblical package, the pronominal suffix is considered as an individual word. This is no longer a problem, as the data of the BHSA has been pre-processed earlier in such a manner that it is similar to the extra-biblical package in this respect. \n",
    "\n",
    "Most functions that were used before on the BHSA can be used again without alterations. Because of the differences mentioned above, only the bundling of usable segments of consecutive words, the collection of input and output data, and the preparation of the test set need to be programmed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dss_blocks(test_book=['B_1QS']):\n",
    "    \n",
    "    dss_blocks = collections.defaultdict(list)\n",
    "    chapters = [\n",
    "        chap for chap in F.otype.s(\"chapter\") if F.book.v(chap) in test_book\n",
    "    ]\n",
    "    \n",
    "    block_index = 0\n",
    "    # iterates over all chapters and collects all words except the elided-he\n",
    "    for chap in chapters:\n",
    "        chap_words = [w for w in L.d(chap, \"word\") if F.g_cons.v(w) != '']\n",
    "        block = []\n",
    "        \n",
    "        # detects and removes omissions and splits blocks when they occur\n",
    "        for word in range(len(chap_words)):\n",
    "            if F.lex.v(chap_words[word]) == '=':\n",
    "                if block != []:\n",
    "                    dss_blocks[block_index] = block\n",
    "            elif F.lex.v(chap_words[word]) != '=':\n",
    "                block.append(chap_words[word])\n",
    "                block_index += 1\n",
    "        if block != []:\n",
    "            dss_blocks[block_index] = block\n",
    "            block_index += 1\n",
    "    \n",
    "    # filters out blocks that are shorter than the sequence length (9)\n",
    "    dss_blocks = {block: words for block, words in dss_blocks.items() if len(words) >= 9}\n",
    "    \n",
    "    # shuffles the blocks randomly\n",
    "    indexes = shuffle(list(dss_blocks.keys()))\n",
    "    dss_blocks = {k: dss_blocks[k] for k in indexes}\n",
    "    \n",
    "    return dss_blocks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dss_data(dss_blocks, ratio=0.9):\n",
    "\n",
    "    dss_data = {}\n",
    "\n",
    "    # iterates through all blocks\n",
    "    for block_idx, block_words in dss_blocks.items():\n",
    "        block_data = []\n",
    "\n",
    "        # iterates through all words\n",
    "        for w in block_words:\n",
    "            block_data.append([position_in_phrase_atom(w), get_pos(w), w])\n",
    "        dss_data[block_idx] = block_data\n",
    "\n",
    "    return dss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_test_data(dss_data):\n",
    "\n",
    "    ip_pos_dss = {}\n",
    "    op_ph_dss = {}\n",
    "    \n",
    "    # iterates through dss blocks\n",
    "    for block in dss_data:\n",
    "        ip_pos_dss_block = []\n",
    "        op_ph_dss_block = []\n",
    "        dss_words = dss_data[block]\n",
    "\n",
    "        for w in range(len(dss_words[:-8])):\n",
    "            \n",
    "            # collects dss data\n",
    "            pos = [dss_words[w][1] for w in range(w, w + 9)]\n",
    "            ph_atom = [dss_words[w][0] for w in range(w, w + 9)]\n",
    "            \n",
    "            ip_pos_dss_block.append(pos)\n",
    "            op_ph_dss_block.append(ph_atom)\n",
    "\n",
    "        ip_pos_dss[block] = ip_pos_dss_block\n",
    "        op_ph_dss[block] = op_ph_dss_block\n",
    "\n",
    "    return ip_pos_dss, op_ph_dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script runs all necessary functions to create the input data the DSS and to run it through the model. The resulting outcomes are shown in tables similar to those of the test set of the BHSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.47\n"
     ]
    }
   ],
   "source": [
    "test_book = ['B_1QS']\n",
    "\n",
    "# creates test data\n",
    "dss_blocks = create_dss_blocks(test_book)\n",
    "dss_data = collect_dss_data(dss_blocks)\n",
    "\n",
    "# prepares test data\n",
    "ip_pos_dss, op_ph_dss = prep_test_data(dss_data)\n",
    "\n",
    "# one-hot encodes test data\n",
    "one_hot_dss_data = {\n",
    "    block: one_hot_encode(max_len_ip, max_len_op, ip_pos_voc, op_ph_voc, ip_pos2idx,\n",
    "                   op_ph2idx, ip_pos_dss[block], op_ph_seq)[0]\n",
    "    for block in dss_data\n",
    "}\n",
    "\n",
    "# creates prediction dictionary\n",
    "decision_dict_dss = prediction_dict(dss_data, one_hot_dss_data, op_ph_dss)\n",
    "\n",
    "df_eval_by_pos_dss, df_cross_eval_dss, df_eval_summary_dss, bible_section_dss = test_evaluation(\n",
    "    dss_data, decision_dict_dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct Classifications</th>\n",
       "      <th>Misclassifications</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>9731</td>\n",
       "      <td>570</td>\n",
       "      <td>94.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Correct Classifications  Misclassifications  Accuracy\n",
       "Value                     9731                 570     94.47"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_summary_dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End of Phrase Atom</th>\n",
       "      <th>Not End of Phrase Atom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted as End</th>\n",
       "      <td>5052</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted as Not End</th>\n",
       "      <td>408</td>\n",
       "      <td>4679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      End of Phrase Atom  Not End of Phrase Atom\n",
       "Predicted as End                    5052                     162\n",
       "Predicted as Not End                 408                    4679"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross_eval_dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, the model trained on the BHSA is able to predict the phrase atom boundaries for the Qumran Community Scroll with a 94.47% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total in Test Set</th>\n",
       "      <th>Total Mistakes</th>\n",
       "      <th>Mistakes Percentage</th>\n",
       "      <th>Total 'x' in Test Set</th>\n",
       "      <th>Mistaken for 'p'</th>\n",
       "      <th>Percentage 'x'</th>\n",
       "      <th>Total 'p' in Test Set</th>\n",
       "      <th>Mistaken for 'x'</th>\n",
       "      <th>Percentage 'p'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subs_a</th>\n",
       "      <td>1606</td>\n",
       "      <td>224</td>\n",
       "      <td>13.95</td>\n",
       "      <td>339</td>\n",
       "      <td>193</td>\n",
       "      <td>56.93</td>\n",
       "      <td>1267</td>\n",
       "      <td>31</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conj</th>\n",
       "      <td>1252</td>\n",
       "      <td>144</td>\n",
       "      <td>11.50</td>\n",
       "      <td>238</td>\n",
       "      <td>117</td>\n",
       "      <td>49.16</td>\n",
       "      <td>1014</td>\n",
       "      <td>27</td>\n",
       "      <td>2.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prps</th>\n",
       "      <td>966</td>\n",
       "      <td>49</td>\n",
       "      <td>5.07</td>\n",
       "      <td>66</td>\n",
       "      <td>41</td>\n",
       "      <td>62.12</td>\n",
       "      <td>900</td>\n",
       "      <td>8</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subs_c</th>\n",
       "      <td>1979</td>\n",
       "      <td>47</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1886</td>\n",
       "      <td>8</td>\n",
       "      <td>0.42</td>\n",
       "      <td>93</td>\n",
       "      <td>39</td>\n",
       "      <td>41.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb_c</th>\n",
       "      <td>104</td>\n",
       "      <td>47</td>\n",
       "      <td>45.19</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>63.16</td>\n",
       "      <td>85</td>\n",
       "      <td>35</td>\n",
       "      <td>41.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>630</td>\n",
       "      <td>16</td>\n",
       "      <td>2.54</td>\n",
       "      <td>546</td>\n",
       "      <td>8</td>\n",
       "      <td>1.47</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>9.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjv_a</th>\n",
       "      <td>185</td>\n",
       "      <td>12</td>\n",
       "      <td>6.49</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>76.92</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advb</th>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>25.53</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>100.00</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>14.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb_a</th>\n",
       "      <td>551</td>\n",
       "      <td>6</td>\n",
       "      <td>1.09</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>85.71</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prep</th>\n",
       "      <td>1940</td>\n",
       "      <td>5</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>71.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prde</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>6.90</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.00</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intj</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmpr_a</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Total in Test Set  Total Mistakes  Mistakes Percentage  \\\n",
       "subs_a               1606             224                13.95   \n",
       "conj                 1252             144                11.50   \n",
       "prps                  966              49                 5.07   \n",
       "subs_c               1979              47                 2.37   \n",
       "verb_c                104              47                45.19   \n",
       "art                   630              16                 2.54   \n",
       "adjv_a                185              12                 6.49   \n",
       "advb                   47              12                25.53   \n",
       "verb_a                551               6                 1.09   \n",
       "prep                 1940               5                 0.26   \n",
       "prde                   58               4                 6.90   \n",
       "intj                    6               3                50.00   \n",
       "nmpr_a                 55               1                 1.82   \n",
       "\n",
       "        Total 'x' in Test Set  Mistaken for 'p'  Percentage 'x'  \\\n",
       "subs_a                    339               193           56.93   \n",
       "conj                      238               117           49.16   \n",
       "prps                       66                41           62.12   \n",
       "subs_c                   1886                 8            0.42   \n",
       "verb_c                     19                12           63.16   \n",
       "art                       546                 8            1.47   \n",
       "adjv_a                     13                10           76.92   \n",
       "advb                        6                 6          100.00   \n",
       "verb_a                      7                 6           85.71   \n",
       "prep                     1933                 0            0.00   \n",
       "prde                        4                 4          100.00   \n",
       "intj                        3                 3          100.00   \n",
       "nmpr_a                      3                 0            0.00   \n",
       "\n",
       "        Total 'p' in Test Set  Mistaken for 'x'  Percentage 'p'  \n",
       "subs_a                   1267                31            2.45  \n",
       "conj                     1014                27            2.66  \n",
       "prps                      900                 8            0.89  \n",
       "subs_c                     93                39           41.94  \n",
       "verb_c                     85                35           41.18  \n",
       "art                        84                 8            9.52  \n",
       "adjv_a                    172                 2            1.16  \n",
       "advb                       41                 6           14.63  \n",
       "verb_a                    544                 0            0.00  \n",
       "prep                        7                 5           71.43  \n",
       "prde                       54                 0            0.00  \n",
       "intj                        3                 0            0.00  \n",
       "nmpr_a                     52                 1            1.92  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_by_pos_dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the mistakes occurred in predicting the phrase atom position for words that were conjunctions or substantives with an absolute state (144 and 224 errors). Relatively, the most errors occurred for verbs in the construct state and adverbs (45.19 and 25.53%). The high error rate of interjections is not meaningful as interjections only occur 6 times in 1QS of which 3 are wrongly predicted. The results are strikingly similar to the evaluation of the test set of the BHSA. This could mean that the Hebrew of 1QS is not that different from the Hebrew of the BHSA. \n",
    "\n",
    "In conclusion, a sequence to sequence Neural Network with a LSTM encoder-decoder is quite capable of finding relations between parts of speech and phrase atom end. For further research, one could build upon this model to predict phrase functions, for instance. In fact, these kind of models could be used in the field of ancient languages for many more applications, such as manuscript clustering, feature parsing, or to address questions of authorship, dating, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completeness, here follows the complete list of wrong predictions both on the test set of the BHSA and on 1QS. Each line shows the node, verse, part of speech, correct and predicted position in the phrase atom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223011 Isaiah 33:6 conj x p\n",
      "223074 Isaiah 33:12 subs_a x p\n",
      "223093 Isaiah 33:14 subs_a p x\n",
      "223164 Isaiah 33:19 subs_a x p\n",
      "223173 Isaiah 33:19 verb_c x p\n",
      "223175 Isaiah 33:19 subs_c p x\n",
      "223199 Isaiah 33:21 conj x p\n",
      "223207 Isaiah 33:21 subs_a x p\n",
      "223242 Isaiah 33:23 subs_a p x\n",
      "223256 Isaiah 33:24 verb_c x p\n",
      "158210 1 Samuel 27:1 subs_a x p\n",
      "158244 1 Samuel 27:2 prps x p\n",
      "158245 1 Samuel 27:2 conj x p\n",
      "158267 1 Samuel 27:3 subs_a x p\n",
      "158268 1 Samuel 27:3 conj x p\n",
      "158270 1 Samuel 27:3 nmpr_a x p\n",
      "158271 1 Samuel 27:3 conj x p\n",
      "97458 Deuteronomy 7:1 subs_a p x\n",
      "97514 Deuteronomy 7:5 conj x p\n",
      "97618 Deuteronomy 7:9 subs_a p x\n",
      "97620 Deuteronomy 7:9 subs_a x p\n",
      "97621 Deuteronomy 7:9 art x p\n",
      "97625 Deuteronomy 7:9 subs_a x p\n",
      "97626 Deuteronomy 7:9 conj x p\n",
      "97630 Deuteronomy 7:9 verb_c x p\n",
      "97630 Deuteronomy 7:9 prps x p\n",
      "97631 Deuteronomy 7:9 conj x p\n",
      "97633 Deuteronomy 7:9 verb_c x p\n",
      "97641 Deuteronomy 7:10 verb_c x p\n",
      "97649 Deuteronomy 7:10 verb_c x p\n",
      "97761 Deuteronomy 7:15 nmpr_a x p\n",
      "97823 Deuteronomy 7:18 subs_a x p\n",
      "97824 Deuteronomy 7:18 conj x p\n",
      "97880 Deuteronomy 7:20 verb_a x p\n",
      "97881 Deuteronomy 7:20 conj x p\n",
      "97882 Deuteronomy 7:20 art x p\n",
      "97910 Deuteronomy 7:22 subs_a x p\n",
      "97963 Deuteronomy 7:25 subs_a x p\n",
      "97964 Deuteronomy 7:25 conj x p\n",
      "282432 Ezekiel 36:2 art x p\n",
      "282453 Ezekiel 36:3 subs_c p x\n",
      "282471 Ezekiel 36:3 subs_a x p\n",
      "282472 Ezekiel 36:3 conj x p\n",
      "282480 Ezekiel 36:4 nmpr_a x p\n",
      "282492 Ezekiel 36:4 subs_a x p\n",
      "282504 Ezekiel 36:4 art p x\n",
      "282540 Ezekiel 36:5 subs_a x p\n",
      "282541 Ezekiel 36:5 conj x p\n",
      "282555 Ezekiel 36:5 subs_a x p\n",
      "282559 Ezekiel 36:5 prep p x\n",
      "282576 Ezekiel 36:6 subs_a x p\n",
      "282590 Ezekiel 36:6 prps x p\n",
      "282591 Ezekiel 36:6 conj x p\n",
      "282595 Ezekiel 36:6 subs_c p x\n",
      "282730 Ezekiel 36:14 nmpr_a x p\n",
      "282752 Ezekiel 36:15 nmpr_a x p\n",
      "282812 Ezekiel 36:19 prps x p\n",
      "282813 Ezekiel 36:19 conj x p\n",
      "282870 Ezekiel 36:22 conj x p\n",
      "282906 Ezekiel 36:23 nmpr_a x p\n",
      "283033 Ezekiel 36:30 subs_a x p\n",
      "283034 Ezekiel 36:30 conj x p\n",
      "283064 Ezekiel 36:31 prps x p\n",
      "283065 Ezekiel 36:31 conj x p\n",
      "283073 Ezekiel 36:32 nmpr_a x p\n",
      "283103 Ezekiel 36:33 subs_a p x\n",
      "283104 Ezekiel 36:34 conj p x\n",
      "283106 Ezekiel 36:34 subs_a x p\n",
      "283107 Ezekiel 36:34 art x p\n",
      "283110 Ezekiel 36:34 subs_c x p\n",
      "283137 Ezekiel 36:35 verb_a x p\n",
      "283138 Ezekiel 36:35 conj x p\n",
      "283139 Ezekiel 36:35 art x p\n",
      "283154 Ezekiel 36:36 art p x\n",
      "283157 Ezekiel 36:36 art p x\n",
      "312029 Psalms 11:5 adjv_a x p\n",
      "312030 Psalms 11:5 conj x p\n",
      "312038 Psalms 11:6 subs_a x p\n",
      "363235 Ecclesiastes 12:1 verb_c x p\n",
      "363298 Ecclesiastes 12:3 art p x\n",
      "363326 Ecclesiastes 12:5 advb x p\n",
      "363359 Ecclesiastes 12:5 art x p\n",
      "363412 Ecclesiastes 12:8 subs_a p x\n",
      "363413 Ecclesiastes 12:9 conj p x\n",
      "363414 Ecclesiastes 12:9 subs_a x p\n",
      "363417 Ecclesiastes 12:9 subs_a p x\n",
      "363450 Ecclesiastes 12:11 subs_a x p\n",
      "363456 Ecclesiastes 12:11 verb_a x p\n",
      "363464 Ecclesiastes 12:12 verb_c p x\n",
      "363467 Ecclesiastes 12:12 subs_c p x\n",
      "198188 2 Kings 6:2 subs_a p x\n",
      "198502 2 Kings 6:17 subs_a x p\n",
      "198503 2 Kings 6:17 conj x p\n",
      "198690 2 Kings 6:25 subs_a x p\n",
      "198695 2 Kings 6:25 subs_a x p\n",
      "198698 2 Kings 6:25 subs_a x p\n",
      "198895 2 Kings 6:33 subs_c p x\n",
      "317549 Psalms 43:1 subs_a x p\n",
      "317550 Psalms 43:1 nega x p\n",
      "317554 Psalms 43:1 subs_a x p\n",
      "317555 Psalms 43:1 conj x p\n",
      "317568 Psalms 43:2 subs_a x p\n",
      "317571 Psalms 43:3 prps x p\n",
      "317572 Psalms 43:3 conj x p\n",
      "317612 Psalms 43:5 prps x p\n",
      "317613 Psalms 43:5 conj x p\n",
      "317614 Psalms 43:5 subs_c p x\n",
      "408774 2 Chronicles 5:8 subs_a x p\n",
      "408814 2 Chronicles 5:10 advb x p\n",
      "408862 2 Chronicles 5:12 nmpr_a x p\n",
      "408864 2 Chronicles 5:12 nmpr_a x p\n",
      "408869 2 Chronicles 5:12 prps x p\n",
      "408870 2 Chronicles 5:12 conj x p\n",
      "408879 2 Chronicles 5:12 subs_a x p\n",
      "408880 2 Chronicles 5:12 conj x p\n",
      "408891 2 Chronicles 5:12 subs_a x p\n",
      "408892 2 Chronicles 5:12 conj x p\n",
      "408903 2 Chronicles 5:13 verb_a x p\n",
      "408904 2 Chronicles 5:13 conj x p\n",
      "408910 2 Chronicles 5:13 subs_a x p\n",
      "361488 Ecclesiastes 7:1 subs_c p x\n",
      "361534 Ecclesiastes 7:4 subs_a p x\n",
      "361549 Ecclesiastes 7:6 subs_a x p\n",
      "361570 Ecclesiastes 7:7 subs_a p x\n",
      "361621 Ecclesiastes 7:11 verb_c x p\n",
      "361664 Ecclesiastes 7:14 advb x p\n",
      "361689 Ecclesiastes 7:15 subs_a p x\n",
      "361695 Ecclesiastes 7:15 subs_a p x\n",
      "361719 Ecclesiastes 7:17 prep x p\n",
      "361720 Ecclesiastes 7:17 nega x p\n",
      "361757 Ecclesiastes 7:20 subs_c p x\n",
      "361768 Ecclesiastes 7:21 advb x p\n",
      "361783 Ecclesiastes 7:21 verb_c p x\n",
      "361785 Ecclesiastes 7:22 advb x p\n",
      "361812 Ecclesiastes 7:24 adjv_a x p\n",
      "361817 Ecclesiastes 7:25 prps x p\n",
      "361818 Ecclesiastes 7:25 conj x p\n",
      "361885 Ecclesiastes 7:28 subs_a x p\n",
      "361913 Ecclesiastes 7:29 subs_a x p\n",
      "106824 Deuteronomy 25:2 subs_c x p\n",
      "106883 Deuteronomy 25:5 subs_a x p\n",
      "106904 Deuteronomy 25:6 prps x p\n",
      "106905 Deuteronomy 25:6 art x p\n",
      "107000 Deuteronomy 25:10 verb_c x p\n",
      "107007 Deuteronomy 25:11 subs_a x p\n",
      "107008 Deuteronomy 25:11 conj x p\n",
      "107043 Deuteronomy 25:13 subs_a p x\n",
      "107044 Deuteronomy 25:13 adjv_a x p\n",
      "107045 Deuteronomy 25:13 conj x p\n",
      "107054 Deuteronomy 25:14 subs_a p x\n",
      "107055 Deuteronomy 25:14 adjv_a x p\n",
      "107056 Deuteronomy 25:14 conj x p\n",
      "107059 Deuteronomy 25:15 adjv_a x p\n",
      "107060 Deuteronomy 25:15 conj x p\n",
      "107065 Deuteronomy 25:15 adjv_a x p\n",
      "107066 Deuteronomy 25:15 conj x p\n",
      "107086 Deuteronomy 25:16 verb_c x p\n",
      "107133 Deuteronomy 25:19 subs_c x p\n",
      "107134 Deuteronomy 25:19 verb_c x p\n",
      "1697 Genesis 4:2 verb_c x p\n",
      "1864 Genesis 4:12 verb_a x p\n",
      "1865 Genesis 4:12 conj x p\n",
      "1896 Genesis 4:14 verb_a x p\n",
      "1897 Genesis 4:14 conj x p\n",
      "1904 Genesis 4:14 subs_c p x\n",
      "1905 Genesis 4:14 verb_c p x\n",
      "2034 Genesis 4:22 verb_c x p\n",
      "2049 Genesis 4:23 nmpr_a x p\n",
      "2050 Genesis 4:23 conj x p\n",
      "2070 Genesis 4:24 nmpr_a p x\n",
      "2071 Genesis 4:24 conj p x\n",
      "340094 Job 16:2 verb_c x p\n",
      "340142 Job 16:7 advb x p\n",
      "340247 Job 16:17 prep p x\n",
      "340248 Job 16:17 nega p x\n",
      "340265 Job 16:19 advb x p\n",
      "340277 Job 16:20 verb_c x p\n",
      "77885 Numbers 13:2 subs_a p x\n",
      "78079 Numbers 13:19 subs_a p x\n",
      "78080 Numbers 13:19 conj p x\n",
      "78095 Numbers 13:20 subs_a p x\n",
      "78096 Numbers 13:20 conj p x\n",
      "78137 Numbers 13:22 advb p x\n",
      "78138 Numbers 13:22 nmpr_a x p\n",
      "78152 Numbers 13:22 nmpr_a x p\n",
      "78280 Numbers 13:28 adjv_a x p\n",
      "78294 Numbers 13:29 subs_a p x\n",
      "78295 Numbers 13:29 conj p x\n",
      "78314 Numbers 13:29 subs_a x p\n",
      "78315 Numbers 13:29 conj x p\n",
      "78380 Numbers 13:32 verb_c x p\n",
      "78382 Numbers 13:32 conj p x\n",
      "215185 Isaiah 9:2 subs_c x p\n",
      "215244 Isaiah 9:5 subs_c x p\n",
      "215246 Isaiah 9:5 subs_a x p\n",
      "215255 Isaiah 9:6 subs_a x p\n",
      "215256 Isaiah 9:6 conj x p\n",
      "215259 Isaiah 9:6 subs_c p x\n",
      "215279 Isaiah 9:6 advb x p\n",
      "215280 Isaiah 9:6 conj x p\n",
      "215284 Isaiah 9:6 nmpr_a x p\n",
      "215302 Isaiah 9:8 nmpr_a x p\n",
      "215303 Isaiah 9:8 conj x p\n",
      "215307 Isaiah 9:8 subs_a x p\n",
      "215308 Isaiah 9:8 conj x p\n",
      "215333 Isaiah 9:10 verb_c x p\n",
      "215365 Isaiah 9:12 art p x\n",
      "215366 Isaiah 9:12 verb_c p x\n",
      "215369 Isaiah 9:12 nmpr_a x p\n",
      "215380 Isaiah 9:13 subs_a p x\n",
      "215383 Isaiah 9:13 subs_a p x\n",
      "215385 Isaiah 9:13 subs_a p x\n",
      "215386 Isaiah 9:14 adjv_a x p\n",
      "215387 Isaiah 9:14 conj x p\n",
      "215409 Isaiah 9:15 verb_c x p\n",
      "215420 Isaiah 9:16 prps x p\n",
      "215421 Isaiah 9:16 conj x p\n",
      "215428 Isaiah 9:16 adjv_a x p\n",
      "215429 Isaiah 9:16 conj x p\n",
      "215452 Isaiah 9:17 subs_a x p\n",
      "215453 Isaiah 9:17 conj x p\n",
      "215468 Isaiah 9:18 nmpr_a x p\n",
      "215503 Isaiah 9:20 nmpr_a p x\n",
      "215504 Isaiah 9:20 conj p x\n",
      "229635 Isaiah 50:1 verb_c x p\n",
      "229652 Isaiah 50:2 subs_c p x\n",
      "229696 Isaiah 50:4 nmpr_a x p\n",
      "229712 Isaiah 50:4 subs_a x p\n",
      "229724 Isaiah 50:5 nmpr_a x p\n",
      "229748 Isaiah 50:6 subs_a x p\n",
      "229749 Isaiah 50:6 conj x p\n",
      "229812 Isaiah 50:10 subs_c p x\n",
      "229827 Isaiah 50:11 verb_c p x\n",
      "342881 Job 29:2 subs_c p x\n",
      "342899 Job 29:4 prep p x\n",
      "342905 Job 29:5 subs_c p x\n",
      "342976 Job 29:13 subs_c x p\n",
      "342989 Job 29:14 subs_a x p\n",
      "342990 Job 29:14 conj x p\n",
      "260262 Jeremiah 47:2 subs_a x p\n",
      "260266 Jeremiah 47:2 subs_a x p\n",
      "260267 Jeremiah 47:2 conj x p\n",
      "260268 Jeremiah 47:2 prps x p\n",
      "260269 Jeremiah 47:2 subs_a x p\n",
      "260270 Jeremiah 47:2 conj x p\n",
      "260271 Jeremiah 47:2 verb_c x p\n",
      "260320 Jeremiah 47:4 subs_a x p\n",
      "260362 Jeremiah 47:7 nmpr_a x p\n",
      "260363 Jeremiah 47:7 conj x p\n",
      "303116 Zephaniah 1:3 subs_a x p\n",
      "303117 Zephaniah 1:3 conj x p\n",
      "303169 Zephaniah 1:5 art p x\n",
      "303180 Zephaniah 1:5 art p x\n",
      "303193 Zephaniah 1:6 art p x\n",
      "303210 Zephaniah 1:7 nmpr_a x p\n",
      "303237 Zephaniah 1:8 subs_a x p\n",
      "303238 Zephaniah 1:8 conj x p\n",
      "303240 Zephaniah 1:8 subs_c p x\n",
      "303241 Zephaniah 1:8 art p x\n",
      "303248 Zephaniah 1:9 subs_c p x\n",
      "303249 Zephaniah 1:9 art p x\n",
      "303362 Zephaniah 1:14 nmpr_a x p\n",
      "303397 Zephaniah 1:15 subs_a p x\n",
      "303406 Zephaniah 1:16 adjv_a x p\n",
      "303407 Zephaniah 1:16 conj x p\n",
      "303438 Zephaniah 1:18 advb x p\n",
      "303439 Zephaniah 1:18 prps x p\n",
      "303440 Zephaniah 1:18 advb x p\n",
      "303459 Zephaniah 1:18 subs_a x p\n",
      "303460 Zephaniah 1:18 advb x p\n",
      "313947 Psalms 23:6 advb x p\n",
      "313948 Psalms 23:6 adjv_a x p\n",
      "313949 Psalms 23:6 conj x p\n",
      "293061 Hosea 10:1 subs_a x p\n",
      "293093 Hosea 10:3 subs_c p x\n",
      "293131 Hosea 10:5 prps x p\n",
      "293132 Hosea 10:5 conj x p\n",
      "293141 Hosea 10:6 advb x p\n",
      "293222 Hosea 10:11 subs_a x p\n",
      "293250 Hosea 10:12 subs_a p x\n",
      "293251 Hosea 10:12 conj p x\n",
      "312693 Psalms 18:3 prps x p\n",
      "312700 Psalms 18:4 verb_c x p\n",
      "312789 Psalms 18:12 subs_a x p\n",
      "312797 Psalms 18:13 subs_a x p\n",
      "312798 Psalms 18:13 conj x p\n",
      "312811 Psalms 18:14 subs_a x p\n",
      "312812 Psalms 18:14 conj x p\n",
      "312821 Psalms 18:15 subs_a p x\n",
      "312850 Psalms 18:18 verb_c x p\n",
      "312850 Psalms 18:18 prps x p\n",
      "312851 Psalms 18:18 adjv_a x p\n",
      "312852 Psalms 18:18 conj x p\n",
      "312854 Psalms 18:18 verb_c x p\n",
      "312929 Psalms 18:26 subs_a x p\n",
      "312992 Psalms 18:33 art p x\n",
      "313139 Psalms 18:49 verb_c x p\n",
      "313140 Psalms 18:49 advb x p\n",
      "313142 Psalms 18:49 verb_c x p\n",
      "291182 Hosea 2:5 subs_c p x\n",
      "291183 Hosea 2:5 verb_c p x\n",
      "291218 Hosea 2:7 verb_c x p\n",
      "291221 Hosea 2:7 prps x p\n",
      "291224 Hosea 2:7 prps x p\n",
      "291247 Hosea 2:9 verb_c x p\n",
      "291335 Hosea 2:13 prps x p\n",
      "291336 Hosea 2:13 prps x p\n",
      "291408 Hosea 2:17 advb p x\n",
      "291409 Hosea 2:17 conj p x\n",
      "291421 Hosea 2:17 prps x p\n",
      "291422 Hosea 2:17 conj x p\n",
      "291424 Hosea 2:17 subs_c p x\n",
      "291477 Hosea 2:20 subs_a x p\n",
      "291478 Hosea 2:20 conj x p\n",
      "412657 2 Chronicles 12:3 subs_a x p\n",
      "412658 2 Chronicles 12:3 conj x p\n",
      "412660 2 Chronicles 12:3 subs_a x p\n",
      "412664 2 Chronicles 12:3 subs_c p x\n",
      "412697 2 Chronicles 12:5 nmpr_a x p\n",
      "412698 2 Chronicles 12:5 conj x p\n",
      "412728 2 Chronicles 12:6 nmpr_a x p\n",
      "412775 2 Chronicles 12:8 prps x p\n",
      "281533 Ezekiel 34:2 verb_c x p\n",
      "281558 Ezekiel 34:2 art x p\n",
      "281668 Ezekiel 34:8 verb_c p x\n",
      "281693 Ezekiel 34:8 art x p\n",
      "281730 Ezekiel 34:10 art x p\n",
      "281758 Ezekiel 34:12 subs_c p x\n",
      "281808 Ezekiel 34:13 subs_a x p\n",
      "281809 Ezekiel 34:13 conj x p\n",
      "281846 Ezekiel 34:15 nmpr_a x p\n",
      "281919 Ezekiel 34:18 art x p\n",
      "281945 Ezekiel 34:20 adjv_a x p\n",
      "281946 Ezekiel 34:20 conj x p\n",
      "281952 Ezekiel 34:21 subs_a x p\n",
      "281953 Ezekiel 34:21 conj x p\n",
      "281961 Ezekiel 34:21 subs_c x p\n",
      "281962 Ezekiel 34:21 art x p\n",
      "281990 Ezekiel 34:23 verb_a x p\n",
      "282049 Ezekiel 34:26 prps x p\n",
      "282050 Ezekiel 34:26 conj x p\n",
      "282129 Ezekiel 34:29 verb_c x p\n",
      "282154 Ezekiel 34:30 nmpr_a x p\n",
      "282166 Ezekiel 34:31 nmpr_a x p\n",
      "319818 Psalms 57:4 prps x p\n",
      "319819 Psalms 57:4 conj x p\n",
      "319873 Psalms 57:9 subs_a x p\n",
      "319874 Psalms 57:9 conj x p\n",
      "58907 Leviticus 13:2 subs_c x p\n",
      "58958 Leviticus 13:4 subs_c p x\n",
      "59005 Leviticus 13:5 subs_a p x\n",
      "59107 Leviticus 13:10 subs_a p x\n",
      "59116 Leviticus 13:11 subs_a x p\n",
      "59308 Leviticus 13:21 adjv_a p x\n",
      "59309 Leviticus 13:21 conj p x\n",
      "59311 Leviticus 13:21 subs_c p x\n",
      "59380 Leviticus 13:25 subs_a p x\n",
      "59420 Leviticus 13:26 subs_c p x\n",
      "59524 Leviticus 13:30 subs_a x p\n",
      "59525 Leviticus 13:30 conj x p\n",
      "59540 Leviticus 13:31 subs_c p x\n",
      "59589 Leviticus 13:32 subs_c p x\n",
      "59610 Leviticus 13:33 subs_a p x\n",
      "59635 Leviticus 13:34 subs_c p x\n",
      "59727 Leviticus 13:39 adjv_a x p\n",
      "59768 Leviticus 13:42 adjv_a x p\n",
      "59770 Leviticus 13:42 subs_a x p\n",
      "59800 Leviticus 13:44 subs_a x p\n",
      "59831 Leviticus 13:45 adjv_a x p\n",
      "59835 Leviticus 13:46 subs_c p x\n",
      "59861 Leviticus 13:47 subs_a x p\n",
      "59865 Leviticus 13:47 subs_a p x\n",
      "59866 Leviticus 13:48 conj p x\n",
      "59878 Leviticus 13:48 subs_a p x\n",
      "59879 Leviticus 13:48 conj p x\n",
      "59973 Leviticus 13:51 subs_a x p\n",
      "59998 Leviticus 13:52 subs_a p x\n",
      "59999 Leviticus 13:52 conj p x\n",
      "60011 Leviticus 13:52 subs_a x p\n",
      "60059 Leviticus 13:54 subs_a p x\n",
      "60065 Leviticus 13:55 subs_c x p\n",
      "60106 Leviticus 13:56 subs_c x p\n",
      "60157 Leviticus 13:57 subs_a p x\n",
      "60158 Leviticus 13:58 conj p x\n",
      "233979 Isaiah 65:2 subs_a x p\n",
      "234016 Isaiah 65:4 art p x\n",
      "234049 Isaiah 65:6 conj x p\n",
      "234075 Isaiah 65:7 prps x p\n",
      "234114 Isaiah 65:9 verb_c x p\n",
      "234244 Isaiah 65:15 nmpr_a x p\n",
      "234287 Isaiah 65:17 adjv_a x p\n",
      "234288 Isaiah 65:17 conj x p\n",
      "234301 Isaiah 65:18 conj x p\n",
      "234334 Isaiah 65:19 subs_a x p\n",
      "234335 Isaiah 65:19 conj x p\n",
      "234344 Isaiah 65:20 subs_a x p\n",
      "234356 Isaiah 65:20 subs_a x p\n",
      "234360 Isaiah 65:20 art x p\n",
      "234363 Isaiah 65:20 subs_a x p\n",
      "234414 Isaiah 65:23 prps p x\n",
      "234415 Isaiah 65:23 conj p x\n",
      "234431 Isaiah 65:25 subs_a x p\n",
      "234432 Isaiah 65:25 conj x p\n",
      "240106 Jeremiah 10:13 subs_c p x\n",
      "240107 Jeremiah 10:13 verb_c p x\n",
      "240169 Jeremiah 10:16 nmpr_a x p\n",
      "240187 Jeremiah 10:18 verb_c x p\n",
      "240209 Jeremiah 10:19 advb p x\n",
      "240223 Jeremiah 10:20 subs_c p x\n",
      "240284 Jeremiah 10:24 advb x p\n",
      "88943 Numbers 31:5 subs_a x p\n",
      "88945 Numbers 31:5 subs_a x p\n",
      "88946 Numbers 31:5 verb_c x p\n",
      "88959 Numbers 31:6 prps x p\n",
      "88960 Numbers 31:6 conj x p\n",
      "88973 Numbers 31:6 subs_a x p\n",
      "88974 Numbers 31:6 conj x p\n",
      "89036 Numbers 31:9 prps p x\n",
      "89037 Numbers 31:9 conj p x\n",
      "89044 Numbers 31:9 prps x p\n",
      "89045 Numbers 31:9 conj x p\n",
      "89115 Numbers 31:12 nmpr_a x p\n",
      "89141 Numbers 31:14 verb_c x p\n",
      "89146 Numbers 31:14 subs_a x p\n",
      "89244 Numbers 31:19 adjv_a x p\n",
      "89245 Numbers 31:19 conj x p\n",
      "89253 Numbers 31:19 prps p x\n",
      "89254 Numbers 31:20 conj p x\n",
      "89264 Numbers 31:20 subs_a x p\n",
      "89265 Numbers 31:20 conj x p\n",
      "89300 Numbers 31:22 subs_a x p\n",
      "89303 Numbers 31:22 subs_a x p\n",
      "89306 Numbers 31:22 subs_a x p\n",
      "89327 Numbers 31:23 advb x p\n",
      "89375 Numbers 31:26 subs_a x p\n",
      "89376 Numbers 31:26 conj x p\n",
      "89380 Numbers 31:26 prps x p\n",
      "89381 Numbers 31:26 conj x p\n",
      "89462 Numbers 31:30 subs_a x p\n",
      "89469 Numbers 31:30 subs_a x p\n",
      "89472 Numbers 31:30 subs_a x p\n",
      "89479 Numbers 31:30 subs_a x p\n",
      "89490 Numbers 31:30 verb_c x p\n",
      "89519 Numbers 31:32 subs_a x p\n",
      "89528 Numbers 31:32 subs_a p x\n",
      "89529 Numbers 31:33 conj p x\n",
      "89534 Numbers 31:33 subs_a p x\n",
      "89535 Numbers 31:34 conj p x\n",
      "89570 Numbers 31:36 subs_a x p\n",
      "89572 Numbers 31:36 subs_a x p\n",
      "89610 Numbers 31:38 subs_a p x\n",
      "89611 Numbers 31:39 conj p x\n",
      "89612 Numbers 31:39 subs_a p x\n",
      "89625 Numbers 31:40 conj p x\n",
      "89655 Numbers 31:41 nmpr_a p x\n",
      "89656 Numbers 31:42 conj p x\n",
      "89682 Numbers 31:43 subs_a x p\n",
      "89687 Numbers 31:43 subs_a p x\n",
      "89688 Numbers 31:44 conj p x\n",
      "89689 Numbers 31:44 subs_a p x\n",
      "89693 Numbers 31:44 subs_a p x\n",
      "89694 Numbers 31:45 conj p x\n",
      "89695 Numbers 31:45 subs_a p x\n",
      "89697 Numbers 31:45 subs_a x p\n",
      "89700 Numbers 31:45 subs_a p x\n",
      "89701 Numbers 31:46 conj p x\n",
      "89716 Numbers 31:47 verb_a x p\n",
      "89734 Numbers 31:47 verb_c x p\n",
      "89790 Numbers 31:50 subs_a x p\n",
      "89795 Numbers 31:50 subs_a x p\n",
      "89796 Numbers 31:50 conj x p\n",
      "89832 Numbers 31:52 subs_a x p\n",
      "89842 Numbers 31:52 subs_a x p\n",
      "89843 Numbers 31:52 conj x p\n",
      "89869 Numbers 31:54 subs_a x p\n",
      "218594 Isaiah 20:1 subs_c p x\n",
      "218672 Isaiah 20:4 nmpr_a x p\n",
      "218673 Isaiah 20:4 conj x p\n",
      "218679 Isaiah 20:4 adjv_a p x\n",
      "218701 Isaiah 20:6 verb_a x p\n",
      "1180 Genesis 3:1 nmpr_a x p\n",
      "1292 Genesis 3:6 advb x p\n",
      "1319 Genesis 3:8 nmpr_a x p\n",
      "1332 Genesis 3:8 subs_a x p\n",
      "1333 Genesis 3:8 conj x p\n",
      "1337 Genesis 3:8 nmpr_a x p\n",
      "1346 Genesis 3:9 nmpr_a x p\n",
      "1408 Genesis 3:13 nmpr_a x p\n",
      "1427 Genesis 3:14 nmpr_a x p\n",
      "1440 Genesis 3:14 subs_a x p\n",
      "1441 Genesis 3:14 conj x p\n",
      "1459 Genesis 3:15 prps x p\n",
      "1460 Genesis 3:15 conj x p\n",
      "1463 Genesis 3:15 subs_a x p\n",
      "1483 Genesis 3:16 prps x p\n",
      "1484 Genesis 3:16 conj x p\n",
      "1577 Genesis 3:21 nmpr_a x p\n",
      "1580 Genesis 3:21 subs_a x p\n",
      "1581 Genesis 3:21 conj x p\n",
      "1590 Genesis 3:22 nmpr_a x p\n",
      "1611 Genesis 3:22 advb x p\n",
      "1624 Genesis 3:23 nmpr_a x p\n",
      "160566 2 Samuel 1:4 advb x p\n",
      "160611 2 Samuel 1:6 subs_a x p\n",
      "160612 2 Samuel 1:6 conj x p\n",
      "160666 2 Samuel 1:10 verb_c p x\n",
      "160692 2 Samuel 1:11 advb x p\n",
      "160742 2 Samuel 1:13 subs_a x p\n",
      "160877 2 Samuel 1:22 adjv_a x p\n",
      "160896 2 Samuel 1:23 verb_a x p\n",
      "160897 2 Samuel 1:23 conj x p\n",
      "160901 2 Samuel 1:23 prps x p\n",
      "160902 2 Samuel 1:23 conj x p\n",
      "160918 2 Samuel 1:24 art p x\n",
      "332880 Psalms 123:2 prep p x\n",
      "332886 Psalms 123:2 prep p x\n",
      "332912 Psalms 123:4 subs_a p x\n",
      "298706 Jonah 1:7 conj x p\n",
      "298730 Jonah 1:8 conj x p\n",
      "298838 Jonah 1:12 conj x p\n",
      "360622 Ecclesiastes 4:4 subs_a x p\n",
      "360640 Ecclesiastes 4:4 subs_a p x\n",
      "360656 Ecclesiastes 4:6 subs_a x p\n",
      "360657 Ecclesiastes 4:6 subs_a x p\n",
      "360658 Ecclesiastes 4:6 conj x p\n",
      "360670 Ecclesiastes 4:8 subs_a p x\n",
      "360673 Ecclesiastes 4:8 subs_c p x\n",
      "360675 Ecclesiastes 4:8 advb x p\n",
      "360676 Ecclesiastes 4:8 subs_a x p\n",
      "360677 Ecclesiastes 4:8 conj x p\n",
      "360682 Ecclesiastes 4:8 subs_c p x\n",
      "360687 Ecclesiastes 4:8 advb x p\n",
      "360766 Ecclesiastes 4:12 subs_a x p\n",
      "360767 Ecclesiastes 4:12 art x p\n",
      "360775 Ecclesiastes 4:13 adjv_a x p\n",
      "360776 Ecclesiastes 4:13 conj x p\n",
      "360780 Ecclesiastes 4:13 adjv_a x p\n",
      "360798 Ecclesiastes 4:14 advb x p\n",
      "360821 Ecclesiastes 4:16 subs_c p x\n",
      "360865 Ecclesiastes 4:17 subs_c p x\n",
      "197466 2 Kings 5:1 verb_c p x\n",
      "197555 2 Kings 5:5 subs_a x p\n",
      "197560 2 Kings 5:5 subs_a x p\n",
      "197563 2 Kings 5:5 subs_a x p\n",
      "197694 2 Kings 5:10 subs_a x p\n",
      "197830 2 Kings 5:15 subs_c p x\n",
      "197836 2 Kings 5:15 conj x p\n",
      "197875 2 Kings 5:17 subs_a x p\n",
      "197889 2 Kings 5:17 conj x p\n",
      "197970 2 Kings 5:20 conj x p\n",
      "198029 2 Kings 5:22 subs_a x p\n",
      "198042 2 Kings 5:23 subs_a x p\n",
      "198100 2 Kings 5:25 inrg x p\n",
      "198101 2 Kings 5:25 conj x p\n",
      "70915 Numbers 3:1 nmpr_a x p\n",
      "70916 Numbers 3:1 conj x p\n",
      "70934 Numbers 3:2 nmpr_a p x\n",
      "70935 Numbers 3:2 conj p x\n",
      "70945 Numbers 3:3 subs_a x p\n",
      "70946 Numbers 3:3 art x p\n",
      "71033 Numbers 3:8 subs_a x p\n",
      "71034 Numbers 3:8 conj x p\n",
      "71051 Numbers 3:9 nmpr_a x p\n",
      "71052 Numbers 3:9 conj x p\n",
      "71062 Numbers 3:9 nmpr_a p x\n",
      "71063 Numbers 3:10 conj p x\n",
      "71065 Numbers 3:10 nmpr_a x p\n",
      "71066 Numbers 3:10 conj x p\n",
      "71076 Numbers 3:10 adjv_a x p\n",
      "71116 Numbers 3:13 subs_c p x\n",
      "71195 Numbers 3:18 nmpr_a p x\n",
      "71196 Numbers 3:19 conj p x\n",
      "71227 Numbers 3:21 adjv_a x p\n",
      "71228 Numbers 3:21 conj x p\n",
      "71247 Numbers 3:22 verb_c x p\n",
      "71249 Numbers 3:22 subs_a x p\n",
      "71250 Numbers 3:22 conj x p\n",
      "71279 Numbers 3:25 subs_a x p\n",
      "71280 Numbers 3:25 conj x p\n",
      "71283 Numbers 3:25 prps x p\n",
      "71284 Numbers 3:25 conj x p\n",
      "71292 Numbers 3:26 subs_a x p\n",
      "71293 Numbers 3:26 conj x p\n",
      "71299 Numbers 3:26 conj p x\n",
      "71343 Numbers 3:28 subs_a p x\n",
      "71344 Numbers 3:28 conj p x\n",
      "71351 Numbers 3:28 verb_c x p\n",
      "71397 Numbers 3:31 subs_a x p\n",
      "71398 Numbers 3:31 conj x p\n",
      "71420 Numbers 3:33 adjv_a x p\n",
      "71421 Numbers 3:33 conj x p\n",
      "71437 Numbers 3:34 subs_a p x\n",
      "71438 Numbers 3:34 conj p x\n",
      "71467 Numbers 3:36 subs_a x p\n",
      "71479 Numbers 3:36 prps p x\n",
      "71480 Numbers 3:37 conj p x\n",
      "71506 Numbers 3:38 nmpr_a x p\n",
      "71507 Numbers 3:38 conj x p\n",
      "71519 Numbers 3:38 adjv_a x p\n",
      "71541 Numbers 3:39 subs_a p x\n",
      "71542 Numbers 3:39 conj p x\n",
      "71543 Numbers 3:39 subs_a p x\n",
      "71562 Numbers 3:40 subs_a p x\n",
      "71563 Numbers 3:40 conj p x\n",
      "71621 Numbers 3:43 conj p x\n",
      "71624 Numbers 3:43 verb_c x p\n",
      "71698 Numbers 3:47 subs_a x p\n",
      "324867 Psalms 83:4 verb_c x p\n",
      "324888 Psalms 83:7 nmpr_a x p\n",
      "324889 Psalms 83:7 conj x p\n",
      "324890 Psalms 83:7 adjv_a x p\n",
      "324891 Psalms 83:7 nmpr_a x p\n",
      "324892 Psalms 83:7 conj x p\n",
      "324901 Psalms 83:8 verb_c x p\n",
      "324916 Psalms 83:10 nmpr_a x p\n",
      "324918 Psalms 83:10 nmpr_a x p\n",
      "324933 Psalms 83:12 adjv_c p x\n",
      "324938 Psalms 83:12 nmpr_a p x\n",
      "324939 Psalms 83:12 conj p x\n",
      "119448 Joshua 11:1 nmpr_a p x\n",
      "119449 Joshua 11:2 conj p x\n",
      "119452 Joshua 11:2 subs_a p x\n",
      "119453 Joshua 11:2 conj p x\n",
      "119459 Joshua 11:2 conj p x\n",
      "119474 Joshua 11:2 subs_a p x\n",
      "119508 Joshua 11:4 prps x p\n",
      "119509 Joshua 11:4 conj x p\n",
      "119510 Joshua 11:4 subs_a x p\n",
      "119586 Joshua 11:7 nmpr_a x p\n",
      "119587 Joshua 11:7 conj x p\n",
      "119612 Joshua 11:8 nmpr_a x p\n",
      "119613 Joshua 11:8 adjv_a x p\n",
      "119614 Joshua 11:8 conj x p\n",
      "119708 Joshua 11:12 prde x p\n",
      "119709 Joshua 11:12 conj x p\n",
      "119760 Joshua 11:14 advb x p\n",
      "119823 Joshua 11:16 nmpr_a x p\n",
      "119835 Joshua 11:16 nmpr_a x p\n",
      "119836 Joshua 11:16 conj x p\n",
      "119944 Joshua 11:21 nmpr_a x p\n",
      "119946 Joshua 11:21 nmpr_a x p\n",
      "119949 Joshua 11:21 conj p x\n",
      "119970 Joshua 11:22 advb x p\n",
      "119972 Joshua 11:22 nmpr_a x p\n",
      "119974 Joshua 11:22 nmpr_a x p\n",
      "119975 Joshua 11:22 conj x p\n",
      "121035 Joshua 14:2 subs_a x p\n",
      "121036 Joshua 14:2 conj x p\n",
      "121046 Joshua 14:3 subs_a x p\n",
      "121047 Joshua 14:3 conj x p\n",
      "121084 Joshua 14:4 conj x p\n",
      "121259 Joshua 14:10 subs_a x p\n",
      "121260 Joshua 14:10 conj x p\n",
      "121263 Joshua 14:11 subs_c p x\n",
      "121283 Joshua 14:11 subs_a p x\n",
      "121284 Joshua 14:11 conj p x\n",
      "121320 Joshua 14:12 adjv_a x p\n",
      "121361 Joshua 14:14 subs_c x p\n",
      "20926 Genesis 38:6 prps p x\n",
      "20927 Genesis 38:6 conj p x\n",
      "20993 Genesis 38:10 advb x p\n",
      "21040 Genesis 38:12 verb_c x p\n",
      "21042 Genesis 38:12 prps x p\n",
      "21043 Genesis 38:12 conj x p\n",
      "21079 Genesis 38:14 nmpr_a p x\n",
      "21080 Genesis 38:14 conj p x\n",
      "21163 Genesis 38:18 conj p x\n",
      "21197 Genesis 38:20 prps p x\n",
      "21244 Genesis 38:22 advb x p\n",
      "21357 Genesis 38:27 verb_c p x\n",
      "138296 Judges 18:1 subs_c p x\n",
      "138339 Judges 18:2 subs_a x p\n",
      "138372 Judges 18:2 advb p x\n",
      "138383 Judges 18:3 subs_a p x\n",
      "138475 Judges 18:7 verb_a x p\n",
      "138476 Judges 18:7 conj x p\n",
      "138544 Judges 18:10 subs_a x p\n",
      "138558 Judges 18:10 subs_c p x\n",
      "138576 Judges 18:11 nmpr_a x p\n",
      "138577 Judges 18:11 conj x p\n",
      "138633 Judges 18:14 subs_a x p\n",
      "138668 Judges 18:15 subs_a p x\n",
      "138678 Judges 18:16 conj p x\n",
      "138732 Judges 18:17 subs_a x p\n",
      "138788 Judges 18:19 verb_c p x\n",
      "138792 Judges 18:19 subs_a x p\n",
      "138795 Judges 18:19 verb_c p x\n",
      "138926 Judges 18:25 prps x p\n",
      "138968 Judges 18:27 subs_a x p\n",
      "138969 Judges 18:27 verb_a x p\n",
      "138970 Judges 18:27 conj x p\n",
      "139063 Judges 18:30 subs_c p x\n",
      "139064 Judges 18:30 verb_c p x\n",
      "139076 Judges 18:31 subs_c p x\n",
      "50452 Exodus 38:1 subs_a x p\n",
      "50457 Exodus 38:1 subs_a x p\n",
      "50490 Exodus 38:3 subs_a x p\n",
      "50507 Exodus 38:4 subs_a x p\n",
      "50596 Exodus 38:9 subs_a x p\n",
      "50602 Exodus 38:10 prps x p\n",
      "50603 Exodus 38:10 subs_a x p\n",
      "50604 Exodus 38:10 conj x p\n",
      "50605 Exodus 38:10 prps x p\n",
      "50606 Exodus 38:10 subs_a p x\n",
      "50610 Exodus 38:10 subs_a x p\n",
      "50611 Exodus 38:10 conj x p\n",
      "50622 Exodus 38:11 prps x p\n",
      "50623 Exodus 38:11 subs_a x p\n",
      "50624 Exodus 38:11 conj x p\n",
      "50625 Exodus 38:11 prps x p\n",
      "50626 Exodus 38:11 subs_a p x\n",
      "50630 Exodus 38:11 subs_a x p\n",
      "50631 Exodus 38:11 conj x p\n",
      "50650 Exodus 38:12 subs_a x p\n",
      "50651 Exodus 38:12 conj x p\n",
      "50658 Exodus 38:13 subs_a p x\n",
      "50660 Exodus 38:13 subs_a p x\n",
      "50663 Exodus 38:14 subs_a x p\n",
      "50690 Exodus 38:15 subs_a x p\n",
      "50701 Exodus 38:16 subs_a p x\n",
      "50702 Exodus 38:16 subs_a x p\n",
      "50713 Exodus 38:17 subs_a x p\n",
      "50714 Exodus 38:17 conj x p\n",
      "50734 Exodus 38:18 subs_c x p\n",
      "50747 Exodus 38:18 subs_a p x\n",
      "50761 Exodus 38:19 prps x p\n",
      "50762 Exodus 38:19 subs_a x p\n",
      "50763 Exodus 38:19 conj x p\n",
      "50764 Exodus 38:19 prps x p\n",
      "50765 Exodus 38:19 subs_a p x\n",
      "50781 Exodus 38:20 subs_a x p\n",
      "50786 Exodus 38:20 subs_a p x\n",
      "50837 Exodus 38:23 verb_a x p\n",
      "50838 Exodus 38:23 conj x p\n",
      "50851 Exodus 38:23 subs_a x p\n",
      "50852 Exodus 38:23 conj x p\n",
      "50929 Exodus 38:26 subs_a x p\n",
      "50935 Exodus 38:26 subs_a x p\n",
      "50936 Exodus 38:26 subs_a x p\n",
      "50937 Exodus 38:26 conj x p\n",
      "50956 Exodus 38:27 subs_a x p\n",
      "50957 Exodus 38:27 conj x p\n",
      "50975 Exodus 38:28 subs_a x p\n",
      "50976 Exodus 38:28 conj x p\n",
      "50979 Exodus 38:28 subs_a x p\n",
      "50980 Exodus 38:28 conj x p\n",
      "51014 Exodus 38:30 subs_a x p\n",
      "51015 Exodus 38:30 conj x p\n",
      "51044 Exodus 38:31 subs_a x p\n",
      "51045 Exodus 38:31 conj x p\n",
      "51050 Exodus 38:31 subs_a x p\n",
      "51051 Exodus 38:31 conj x p\n",
      "31972 Exodus 7:11 advb x p\n",
      "32149 Exodus 7:19 prps x p\n",
      "32360 Exodus 7:28 prps p x\n",
      "32361 Exodus 7:28 conj p x\n",
      "32363 Exodus 7:28 prps p x\n",
      "32364 Exodus 7:28 conj p x\n",
      "32378 Exodus 7:29 prps x p\n",
      "32381 Exodus 7:29 prps x p\n",
      "32382 Exodus 7:29 conj x p\n",
      "242638 Jeremiah 16:3 subs_a p x\n",
      "242639 Jeremiah 16:3 art p x\n",
      "242645 Jeremiah 16:3 prde p x\n",
      "242646 Jeremiah 16:3 conj p x\n",
      "242681 Jeremiah 16:4 subs_a x p\n",
      "242682 Jeremiah 16:4 conj x p\n",
      "242809 Jeremiah 16:9 prps p x\n",
      "242810 Jeremiah 16:9 conj p x\n",
      "242817 Jeremiah 16:9 subs_a x p\n",
      "242928 Jeremiah 16:13 prps x p\n",
      "242929 Jeremiah 16:13 conj x p\n",
      "242935 Jeremiah 16:13 subs_a x p\n",
      "242937 Jeremiah 16:13 advb x p\n",
      "242938 Jeremiah 16:13 conj x p\n",
      "242954 Jeremiah 16:14 subs_a p x\n",
      "242965 Jeremiah 16:15 conj x p\n",
      "243007 Jeremiah 16:16 adjv_a x p\n",
      "243014 Jeremiah 16:16 subs_a x p\n",
      "243015 Jeremiah 16:16 conj x p\n",
      "243019 Jeremiah 16:16 subs_a x p\n",
      "243020 Jeremiah 16:16 conj x p\n",
      "243046 Jeremiah 16:18 prps x p\n",
      "243047 Jeremiah 16:18 conj x p\n",
      "243055 Jeremiah 16:18 prps x p\n",
      "243056 Jeremiah 16:18 conj x p\n",
      "243078 Jeremiah 16:19 advb x p\n",
      "217754 Isaiah 17:3 nmpr_a x p\n",
      "217774 Isaiah 17:5 subs_a x p\n",
      "217795 Isaiah 17:6 subs_a x p\n",
      "217819 Isaiah 17:7 verb_c x p\n",
      "217855 Isaiah 17:9 verb_c x p\n",
      "217857 Isaiah 17:9 subs_a x p\n",
      "217858 Isaiah 17:9 conj x p\n",
      "217902 Isaiah 17:11 verb_a x p\n",
      "217903 Isaiah 17:11 conj x p\n",
      "217956 Isaiah 17:14 subs_c p x\n",
      "217958 Isaiah 17:14 subs_c p x\n",
      "217961 Isaiah 17:14 prps x p\n",
      "217962 Isaiah 17:14 conj x p\n",
      "217965 Isaiah 17:14 verb_c x p\n",
      "268911 Ezekiel 11:3 art p x\n",
      "268946 Ezekiel 11:5 nmpr_a p x\n",
      "268947 Ezekiel 11:5 conj p x\n",
      "269100 Ezekiel 11:15 prps x p\n",
      "269103 Ezekiel 11:15 prps x p\n",
      "269104 Ezekiel 11:15 conj x p\n",
      "269197 Ezekiel 11:19 subs_a x p\n",
      "269217 Ezekiel 11:20 prep p x\n",
      "269242 Ezekiel 11:21 prps x p\n",
      "269243 Ezekiel 11:21 conj x p\n",
      "114955 Joshua 5:1 conj p x\n",
      "115063 Joshua 5:5 subs_a p x\n",
      "115064 Joshua 5:5 art p x\n",
      "115079 Joshua 5:6 subs_a x p\n",
      "115110 Joshua 5:6 subs_c x p\n",
      "115123 Joshua 5:6 subs_a x p\n",
      "115203 Joshua 5:10 subs_a x p\n",
      "115225 Joshua 5:11 subs_a x p\n",
      "115226 Joshua 5:11 conj x p\n",
      "323551 Psalms 78:8 subs_a x p\n",
      "323552 Psalms 78:8 verb_a x p\n",
      "323553 Psalms 78:8 conj x p\n",
      "323567 Psalms 78:9 verb_c x p\n",
      "323586 Psalms 78:11 prps x p\n",
      "323587 Psalms 78:11 conj x p\n",
      "323630 Psalms 78:15 subs_a p x\n",
      "323689 Psalms 78:20 advb x p\n",
      "323823 Psalms 78:32 verb_c x p\n",
      "323852 Psalms 78:35 verb_c x p\n",
      "323931 Psalms 78:43 prps p x\n",
      "323932 Psalms 78:43 conj p x\n",
      "323941 Psalms 78:44 prps x p\n",
      "323942 Psalms 78:44 conj x p\n",
      "323990 Psalms 78:49 prps x p\n",
      "324048 Psalms 78:53 verb_c x p\n",
      "324082 Psalms 78:56 subs_a p x\n",
      "324218 Psalms 78:70 subs_a x p\n",
      "324238 Psalms 78:72 conj p x\n",
      "194822 2 Kings 1:3 subs_c p x\n",
      "194823 2 Kings 1:3 subs_c p x\n",
      "194892 2 Kings 1:6 subs_c p x\n",
      "194893 2 Kings 1:6 subs_c p x\n",
      "194906 2 Kings 1:6 advb p x\n",
      "194942 2 Kings 1:8 subs_a p x\n",
      "194943 2 Kings 1:8 conj p x\n",
      "194959 2 Kings 1:9 subs_a x p\n",
      "195026 2 Kings 1:11 adjv_a x p\n",
      "195027 2 Kings 1:11 conj x p\n",
      "195084 2 Kings 1:13 adjv_a x p\n",
      "195085 2 Kings 1:13 conj x p\n",
      "195114 2 Kings 1:13 prps x p\n",
      "195117 2 Kings 1:13 prps x p\n",
      "195118 2 Kings 1:13 prde x p\n",
      "195136 2 Kings 1:14 adjv_a x p\n",
      "195137 2 Kings 1:14 conj x p\n",
      "195185 2 Kings 1:16 subs_c p x\n",
      "195186 2 Kings 1:16 subs_c p x\n",
      "321642 Psalms 69:3 subs_c p x\n",
      "321665 Psalms 69:5 verb_c x p\n",
      "321669 Psalms 69:5 verb_c p x\n",
      "321689 Psalms 69:7 verb_c p x\n",
      "321691 Psalms 69:7 nmpr_a x p\n",
      "321696 Psalms 69:7 verb_c p x\n",
      "321758 Psalms 69:14 subs_a p x\n",
      "321775 Psalms 69:15 verb_c x p\n",
      "321775 Psalms 69:15 prps x p\n",
      "321776 Psalms 69:15 conj x p\n",
      "321820 Psalms 69:19 verb_c x p\n",
      "321831 Psalms 69:20 verb_c x p\n",
      "321865 Psalms 69:23 subs_a p x\n",
      "321866 Psalms 69:23 conj p x\n",
      "321924 Psalms 69:30 adjv_a x p\n",
      "321925 Psalms 69:30 conj x p\n",
      "321970 Psalms 69:35 subs_a x p\n",
      "321971 Psalms 69:35 conj x p\n",
      "170298 2 Samuel 17:1 subs_a x p\n",
      "170373 2 Samuel 17:5 advb x p\n",
      "170383 2 Samuel 17:5 advb x p\n",
      "170468 2 Samuel 17:9 subs_a x p\n",
      "170484 2 Samuel 17:9 art p x\n",
      "170573 2 Samuel 17:12 prps x p\n",
      "170574 2 Samuel 17:12 conj x p\n",
      "170581 2 Samuel 17:12 advb x p\n",
      "170609 2 Samuel 17:13 advb x p\n",
      "170613 2 Samuel 17:14 nmpr_a x p\n",
      "170614 2 Samuel 17:14 conj x p\n",
      "170633 2 Samuel 17:14 nmpr_a x p\n",
      "170639 2 Samuel 17:14 verb_c p x\n",
      "170657 2 Samuel 17:15 prde x p\n",
      "170658 2 Samuel 17:15 conj x p\n",
      "170703 2 Samuel 17:16 subs_a x p\n",
      "170704 2 Samuel 17:16 conj x p\n",
      "170827 2 Samuel 17:21 verb_c p x\n",
      "170872 2 Samuel 17:22 verb_c x p\n",
      "170975 2 Samuel 17:27 nmpr_a p x\n",
      "170976 2 Samuel 17:27 nmpr_a p x\n",
      "170977 2 Samuel 17:27 conj p x\n",
      "171016 2 Samuel 17:28 subs_a p x\n",
      "171017 2 Samuel 17:29 conj p x\n",
      "171022 2 Samuel 17:29 subs_a x p\n",
      "171023 2 Samuel 17:29 conj x p\n",
      "294148 Joel 1:6 subs_c p x\n",
      "294173 Joel 1:8 subs_a x p\n",
      "294174 Joel 1:8 verb_c x p\n",
      "294180 Joel 1:9 subs_a x p\n",
      "294181 Joel 1:9 conj x p\n",
      "294222 Joel 1:12 subs_a x p\n",
      "294251 Joel 1:13 verb_c x p\n",
      "294258 Joel 1:13 subs_a x p\n",
      "294259 Joel 1:13 conj x p\n",
      "294266 Joel 1:14 adjv_a x p\n",
      "294322 Joel 1:18 subs_c p x\n",
      "294325 Joel 1:18 advb x p\n",
      "294345 Joel 1:20 advb x p\n",
      "294353 Joel 1:20 subs_a p x\n",
      "294354 Joel 1:20 conj p x\n",
      "217993 Isaiah 18:2 subs_a x p\n",
      "217994 Isaiah 18:2 verb_a x p\n",
      "217995 Isaiah 18:2 conj x p\n",
      "218005 Isaiah 18:2 subs_a x p\n",
      "218015 Isaiah 18:3 subs_a x p\n",
      "218016 Isaiah 18:3 conj x p\n",
      "218058 Isaiah 18:5 subs_a x p\n",
      "218079 Isaiah 18:6 subs_a x p\n",
      "218105 Isaiah 18:7 nmpr_a x p\n",
      "218106 Isaiah 18:7 subs_a p x\n",
      "218107 Isaiah 18:7 subs_a x p\n",
      "218108 Isaiah 18:7 verb_a x p\n",
      "218109 Isaiah 18:7 conj x p\n",
      "218120 Isaiah 18:7 subs_a x p\n",
      "218131 Isaiah 18:7 nmpr_a x p\n",
      "248565 Jeremiah 27:5 subs_a x p\n",
      "248572 Jeremiah 27:5 subs_a p x\n",
      "248573 Jeremiah 27:5 conj p x\n",
      "248581 Jeremiah 27:5 adjv_a x p\n",
      "248582 Jeremiah 27:5 conj x p\n",
      "248584 Jeremiah 27:5 prps x p\n",
      "248595 Jeremiah 27:6 advb p x\n",
      "248702 Jeremiah 27:9 prps x p\n",
      "248703 Jeremiah 27:9 conj x p\n",
      "248708 Jeremiah 27:9 prps x p\n",
      "248709 Jeremiah 27:9 conj x p\n",
      "248789 Jeremiah 27:12 prps x p\n",
      "248790 Jeremiah 27:12 conj x p\n",
      "248796 Jeremiah 27:13 prps x p\n",
      "248797 Jeremiah 27:13 conj x p\n",
      "248801 Jeremiah 27:13 subs_a x p\n",
      "248862 Jeremiah 27:15 prps x p\n",
      "248863 Jeremiah 27:15 conj x p\n",
      "248872 Jeremiah 27:16 subs_a x p\n",
      "248873 Jeremiah 27:16 conj x p\n",
      "248904 Jeremiah 27:16 subs_a p x\n",
      "248905 Jeremiah 27:16 conj p x\n",
      "248939 Jeremiah 27:18 nmpr_a x p\n",
      "248950 Jeremiah 27:18 nmpr_a x p\n",
      "248951 Jeremiah 27:18 conj x p\n",
      "248966 Jeremiah 27:19 subs_a p x\n",
      "248967 Jeremiah 27:19 conj p x\n",
      "249014 Jeremiah 27:21 nmpr_a x p\n",
      "249024 Jeremiah 27:21 nmpr_a x p\n",
      "249025 Jeremiah 27:21 conj x p\n",
      "249030 Jeremiah 27:21 nmpr_a p x\n",
      "122905 Joshua 18:4 subs_a x p\n",
      "122932 Joshua 18:5 subs_a x p\n",
      "122970 Joshua 18:7 subs_c p x\n",
      "123060 Joshua 18:9 subs_a x p\n",
      "123169 Joshua 18:13 nmpr_a x p\n",
      "123179 Joshua 18:14 subs_a p x\n",
      "123201 Joshua 18:14 subs_a p x\n",
      "123202 Joshua 18:15 conj p x\n",
      "123340 Joshua 18:21 nmpr_a p x\n",
      "123341 Joshua 18:22 conj p x\n",
      "123347 Joshua 18:23 conj p x\n",
      "123354 Joshua 18:23 nmpr_a p x\n",
      "123355 Joshua 18:24 conj p x\n",
      "123366 Joshua 18:24 subs_a x p\n",
      "123367 Joshua 18:24 conj x p\n",
      "123369 Joshua 18:25 nmpr_a x p\n",
      "123370 Joshua 18:25 conj x p\n",
      "123374 Joshua 18:25 nmpr_a p x\n",
      "123375 Joshua 18:26 conj p x\n",
      "123383 Joshua 18:26 nmpr_a p x\n",
      "123384 Joshua 18:27 conj p x\n",
      "123389 Joshua 18:27 nmpr_a p x\n",
      "123390 Joshua 18:28 conj p x\n",
      "123391 Joshua 18:28 nmpr_a x p\n",
      "123393 Joshua 18:28 nmpr_a x p\n",
      "123394 Joshua 18:28 conj x p\n",
      "123398 Joshua 18:28 nmpr_a p x\n",
      "123401 Joshua 18:28 subs_a x p\n",
      "123403 Joshua 18:28 subs_a x p\n",
      "123404 Joshua 18:28 conj x p\n",
      "359042 Song of songs 7:14 subs_a p x\n",
      "359043 Song of songs 7:14 adjv_a x p\n",
      "359044 Song of songs 7:14 advb x p\n",
      "34453 Exodus 11:1 subs_a p x\n",
      "34458 Exodus 11:1 subs_a x p\n",
      "34459 Exodus 11:1 conj x p\n",
      "34508 Exodus 11:3 advb x p\n",
      "34512 Exodus 11:3 adjv_a x p\n",
      "34520 Exodus 11:3 subs_a x p\n",
      "34562 Exodus 11:5 subs_a p x\n",
      "34563 Exodus 11:5 conj p x\n",
      "34594 Exodus 11:7 subs_a x p\n",
      "34604 Exodus 11:7 nmpr_a x p\n",
      "34605 Exodus 11:7 conj x p\n",
      "34611 Exodus 11:8 prps x p\n",
      "34620 Exodus 11:8 prps x p\n",
      "34621 Exodus 11:8 conj x p\n",
      "34654 Exodus 11:9 nmpr_a p x\n",
      "34655 Exodus 11:10 conj p x\n",
      "171112 2 Samuel 18:3 conj x p\n",
      "171129 2 Samuel 18:3 advb p x\n",
      "171131 2 Samuel 18:3 subs_a x p\n",
      "171310 2 Samuel 18:9 subs_a x p\n",
      "171314 2 Samuel 18:9 subs_a p x\n",
      "171315 2 Samuel 18:9 conj p x\n",
      "171323 2 Samuel 18:10 subs_a x p\n",
      "171394 2 Samuel 18:12 prps x p\n",
      "171438 2 Samuel 18:14 subs_a x p\n",
      "171447 2 Samuel 18:14 subs_c p x\n",
      "171455 2 Samuel 18:15 subs_a x p\n",
      "171457 2 Samuel 18:15 verb_c x p\n",
      "171506 2 Samuel 18:17 adjv_a x p\n",
      "171537 2 Samuel 18:18 verb_c p x\n",
      "171655 2 Samuel 18:22 subs_c p x\n",
      "171656 2 Samuel 18:22 subs_a x p\n",
      "171740 2 Samuel 18:26 art x p\n",
      "171884 2 Samuel 18:31 subs_c p x\n",
      "171910 2 Samuel 18:32 verb_c x p\n",
      "346964 Proverbs 1:4 subs_a x p\n",
      "346965 Proverbs 1:4 conj x p\n",
      "346978 Proverbs 1:6 subs_a x p\n",
      "346982 Proverbs 1:6 adjv_a x p\n",
      "346983 Proverbs 1:6 conj x p\n",
      "346988 Proverbs 1:7 subs_a p x\n",
      "346989 Proverbs 1:7 subs_a x p\n",
      "346990 Proverbs 1:7 conj x p\n",
      "346991 Proverbs 1:7 subs_a p x\n",
      "347032 Proverbs 1:12 subs_a p x\n",
      "347037 Proverbs 1:12 verb_c x p\n",
      "347050 Proverbs 1:14 subs_a x p\n",
      "347200 Proverbs 1:29 subs_c x p\n",
      "262755 Jeremiah 51:1 verb_c x p\n",
      "262758 Jeremiah 51:1 subs_a x p\n",
      "262779 Jeremiah 51:3 prep x p\n",
      "262781 Jeremiah 51:3 art p x\n",
      "262785 Jeremiah 51:3 prep x p\n",
      "262816 Jeremiah 51:5 nmpr_a x p\n",
      "262960 Jeremiah 51:12 verb_c x p\n",
      "262973 Jeremiah 51:14 nmpr_a x p\n",
      "262977 Jeremiah 51:14 conj x p\n",
      "263002 Jeremiah 51:16 subs_c p x\n",
      "263003 Jeremiah 51:16 verb_c p x\n",
      "263063 Jeremiah 51:19 nmpr_a x p\n",
      "263082 Jeremiah 51:21 subs_a x p\n",
      "263083 Jeremiah 51:21 conj x p\n",
      "263088 Jeremiah 51:21 subs_a x p\n",
      "263089 Jeremiah 51:21 conj x p\n",
      "263112 Jeremiah 51:23 verb_a x p\n",
      "263113 Jeremiah 51:23 conj x p\n",
      "263118 Jeremiah 51:23 subs_a x p\n",
      "263119 Jeremiah 51:23 conj x p\n",
      "263208 Jeremiah 51:27 nmpr_a x p\n",
      "263209 Jeremiah 51:27 nmpr_a x p\n",
      "263210 Jeremiah 51:27 conj x p\n",
      "263225 Jeremiah 51:28 nmpr_a x p\n",
      "263352 Jeremiah 51:35 prps x p\n",
      "263353 Jeremiah 51:35 conj x p\n",
      "263363 Jeremiah 51:35 verb_c x p\n",
      "263393 Jeremiah 51:37 subs_a x p\n",
      "263394 Jeremiah 51:37 subs_a x p\n",
      "263395 Jeremiah 51:37 conj x p\n",
      "263467 Jeremiah 51:43 subs_a x p\n",
      "263468 Jeremiah 51:43 conj x p\n",
      "263469 Jeremiah 51:43 subs_a p x\n",
      "263500 Jeremiah 51:44 advb x p\n",
      "263579 Jeremiah 51:48 subs_a x p\n",
      "263580 Jeremiah 51:48 conj x p\n",
      "263589 Jeremiah 51:48 art x p\n",
      "263599 Jeremiah 51:49 advb x p\n",
      "263718 Jeremiah 51:57 prps x p\n",
      "263719 Jeremiah 51:57 conj x p\n",
      "263720 Jeremiah 51:57 prps x p\n",
      "263736 Jeremiah 51:57 nmpr_a x p\n",
      "263744 Jeremiah 51:58 nmpr_a x p\n",
      "263750 Jeremiah 51:58 prps x p\n",
      "289172 Ezekiel 46:4 adjv_a x p\n",
      "289173 Ezekiel 46:4 conj x p\n",
      "289177 Ezekiel 46:5 subs_a p x\n",
      "289182 Ezekiel 46:5 conj p x\n",
      "289190 Ezekiel 46:5 subs_a p x\n",
      "289200 Ezekiel 46:6 subs_a x p\n",
      "289203 Ezekiel 46:6 adjv_a x p\n",
      "289204 Ezekiel 46:6 conj x p\n",
      "289206 Ezekiel 46:6 subs_a x p\n",
      "289207 Ezekiel 46:6 conj x p\n",
      "289326 Ezekiel 46:11 subs_a p x\n",
      "289327 Ezekiel 46:11 conj p x\n",
      "289334 Ezekiel 46:11 subs_a p x\n",
      "289344 Ezekiel 46:12 subs_a p x\n",
      "289347 Ezekiel 46:12 subs_a p x\n",
      "289382 Ezekiel 46:12 verb_c p x\n",
      "289384 Ezekiel 46:13 subs_a x p\n",
      "289386 Ezekiel 46:13 prps x p\n",
      "289397 Ezekiel 46:13 subs_a x p\n",
      "289409 Ezekiel 46:14 subs_a x p\n",
      "289412 Ezekiel 46:14 subs_a p x\n",
      "289446 Ezekiel 46:15 subs_a x p\n",
      "289624 Ezekiel 46:22 subs_a p x\n",
      "289625 Ezekiel 46:22 subs_a p x\n",
      "289629 Ezekiel 46:22 subs_a p x\n",
      "289630 Ezekiel 46:22 subs_a x p\n",
      "318292 Psalms 48:1 subs_a p x\n",
      "318332 Psalms 48:5 advb p x\n",
      "318359 Psalms 48:9 nmpr_a x p\n",
      "334644 Psalms 140:8 nmpr_a x p\n",
      "334701 Psalms 140:14 advb x p\n",
      "56901 Leviticus 9:2 subs_a x p\n",
      "56903 Leviticus 9:2 subs_a x p\n",
      "56905 Leviticus 9:2 subs_a x p\n",
      "56906 Leviticus 9:2 conj x p\n",
      "56907 Leviticus 9:2 subs_a x p\n",
      "56931 Leviticus 9:3 subs_a x p\n",
      "56936 Leviticus 9:3 subs_a p x\n",
      "56937 Leviticus 9:4 conj p x\n",
      "57013 Leviticus 9:7 prps x p\n",
      "57014 Leviticus 9:7 conj x p\n",
      "57073 Leviticus 9:9 subs_a p x\n",
      "57074 Leviticus 9:10 conj p x\n",
      "57100 Leviticus 9:10 nmpr_a p x\n",
      "57101 Leviticus 9:11 conj p x\n",
      "57104 Leviticus 9:11 subs_a x p\n",
      "57105 Leviticus 9:11 conj x p\n",
      "57145 Leviticus 9:13 conj p x\n",
      "57290 Leviticus 9:21 conj p x\n",
      "57293 Leviticus 9:21 subs_a x p\n",
      "57294 Leviticus 9:21 conj x p\n",
      "57324 Leviticus 9:22 subs_a x p\n",
      "417705 2 Chronicles 23:4 verb_c x p\n",
      "417710 2 Chronicles 23:4 subs_a x p\n",
      "417711 2 Chronicles 23:4 conj x p\n",
      "417746 2 Chronicles 23:6 conj x p\n",
      "417749 2 Chronicles 23:6 subs_a x p\n",
      "417750 2 Chronicles 23:6 conj x p\n",
      "417751 2 Chronicles 23:6 art x p\n",
      "417775 2 Chronicles 23:7 subs_a p x\n",
      "417801 2 Chronicles 23:8 adjv_a x p\n",
      "417802 2 Chronicles 23:8 conj x p\n",
      "417817 2 Chronicles 23:8 verb_c x p\n",
      "417821 2 Chronicles 23:8 verb_c x p\n",
      "417887 2 Chronicles 23:10 subs_a x p\n",
      "417888 2 Chronicles 23:10 conj x p\n",
      "417917 2 Chronicles 23:11 nmpr_a x p\n",
      "417979 2 Chronicles 23:13 art x p\n",
      "417996 2 Chronicles 23:13 subs_a x p\n",
      "418056 2 Chronicles 23:16 prps x p\n",
      "418057 2 Chronicles 23:16 conj x p\n",
      "418061 2 Chronicles 23:16 subs_a x p\n",
      "418062 2 Chronicles 23:16 conj x p\n",
      "418084 2 Chronicles 23:17 prps x p\n",
      "418085 2 Chronicles 23:17 conj x p\n",
      "118393 Joshua 10:2 subs_a p x\n",
      "118394 Joshua 10:2 conj p x\n",
      "118465 Joshua 10:5 nmpr_a x p\n",
      "118467 Joshua 10:5 nmpr_a x p\n",
      "118469 Joshua 10:5 nmpr_a x p\n",
      "118471 Joshua 10:5 nmpr_a x p\n",
      "118527 Joshua 10:7 prps p x\n",
      "118528 Joshua 10:7 conj p x\n",
      "118561 Joshua 10:9 advb p x\n",
      "118638 Joshua 10:12 subs_c p x\n",
      "118706 Joshua 10:14 prps x p\n",
      "118707 Joshua 10:14 conj x p\n",
      "118804 Joshua 10:20 nmpr_a x p\n",
      "118805 Joshua 10:20 conj x p\n",
      "118811 Joshua 10:20 adjv_a x p\n",
      "118885 Joshua 10:23 nmpr_a x p\n",
      "118888 Joshua 10:23 nmpr_a x p\n",
      "118891 Joshua 10:23 nmpr_a x p\n",
      "118894 Joshua 10:23 nmpr_a x p\n",
      "118977 Joshua 10:26 subs_a x p\n",
      "118991 Joshua 10:27 subs_c p x\n",
      "118992 Joshua 10:27 verb_c p x\n",
      "119044 Joshua 10:28 prps x p\n",
      "119045 Joshua 10:28 conj x p\n",
      "119083 Joshua 10:30 advb x p\n",
      "119087 Joshua 10:30 nmpr_a p x\n",
      "119088 Joshua 10:30 conj p x\n",
      "119095 Joshua 10:30 subs_a p x\n",
      "119096 Joshua 10:30 conj p x\n",
      "119152 Joshua 10:32 subs_a p x\n",
      "119153 Joshua 10:32 conj p x\n",
      "119213 Joshua 10:35 subs_a p x\n",
      "119214 Joshua 10:35 conj p x\n",
      "119252 Joshua 10:37 subs_a p x\n",
      "119253 Joshua 10:37 conj p x\n",
      "119278 Joshua 10:37 prps x p\n",
      "119279 Joshua 10:37 conj x p\n",
      "119386 Joshua 10:41 nmpr_a p x\n",
      "119387 Joshua 10:41 conj p x\n",
      "119391 Joshua 10:41 nmpr_a p x\n",
      "119392 Joshua 10:41 conj p x\n",
      "119394 Joshua 10:41 nmpr_a p x\n",
      "119395 Joshua 10:42 conj p x\n",
      "119401 Joshua 10:42 prde x p\n",
      "119402 Joshua 10:42 conj x p\n",
      "321229 Psalms 68:1 subs_a p x\n",
      "321303 Psalms 68:7 advb x p\n",
      "321320 Psalms 68:9 advb x p\n",
      "321355 Psalms 68:12 art x p\n",
      "321401 Psalms 68:17 subs_a x p\n",
      "321409 Psalms 68:17 advb x p\n",
      "321415 Psalms 68:18 subs_a p x\n",
      "321437 Psalms 68:19 advb x p\n",
      "321441 Psalms 68:19 nmpr_a x p\n",
      "321461 Psalms 68:21 nmpr_a x p\n",
      "321467 Psalms 68:22 advb x p\n",
      "321510 Psalms 68:26 subs_c p x\n",
      "321511 Psalms 68:26 subs_a x p\n",
      "101040 Deuteronomy 13:2 subs_a x p\n",
      "101041 Deuteronomy 13:2 conj x p\n",
      "101078 Deuteronomy 13:4 prps x p\n",
      "101079 Deuteronomy 13:4 conj x p\n",
      "101081 Deuteronomy 13:4 verb_c x p\n",
      "101094 Deuteronomy 13:4 subs_c p x\n",
      "101131 Deuteronomy 13:6 prps x p\n",
      "101132 Deuteronomy 13:6 conj x p\n",
      "101152 Deuteronomy 13:6 art p x\n",
      "101179 Deuteronomy 13:7 prps p x\n",
      "101180 Deuteronomy 13:7 conj p x\n",
      "101205 Deuteronomy 13:7 prps x p\n",
      "101206 Deuteronomy 13:7 conj x p\n",
      "101218 Deuteronomy 13:8 art p x\n",
      "101335 Deuteronomy 13:14 verb_c x p\n",
      "101370 Deuteronomy 13:16 verb_c x p\n",
      "101379 Deuteronomy 13:16 prps x p\n",
      "101406 Deuteronomy 13:17 subs_a x p\n",
      "101407 Deuteronomy 13:17 conj x p\n",
      "353035 Proverbs 23:5 subs_c p x\n",
      "353062 Proverbs 23:7 advb p x\n",
      "353078 Proverbs 23:8 prps x p\n",
      "353079 Proverbs 23:8 art x p\n",
      "353162 Proverbs 23:17 conj x p\n",
      "353170 Proverbs 23:18 conj x p\n",
      "353172 Proverbs 23:18 subs_a p x\n",
      "353192 Proverbs 23:20 verb_c x p\n",
      "353199 Proverbs 23:21 verb_a x p\n",
      "353200 Proverbs 23:21 conj x p\n",
      "353225 Proverbs 23:23 subs_a x p\n",
      "353226 Proverbs 23:23 conj x p\n",
      "353288 Proverbs 23:29 subs_a x p\n",
      "359305 Ecclesiastes 1:4 subs_a p x\n",
      "359359 Ecclesiastes 1:7 subs_c p x\n",
      "359406 Ecclesiastes 1:9 subs_c p x\n",
      "359412 Ecclesiastes 1:10 subs_a p x\n",
      "359429 Ecclesiastes 1:11 subs_c p x\n",
      "359435 Ecclesiastes 1:11 advb x p\n",
      "49214 Exodus 36:2 subs_c p x\n",
      "49256 Exodus 36:3 subs_a x p\n",
      "49314 Exodus 36:6 subs_a x p\n",
      "49315 Exodus 36:6 conj x p\n",
      "49351 Exodus 36:8 verb_c x p\n",
      "49359 Exodus 36:8 subs_a x p\n",
      "49360 Exodus 36:8 verb_a x p\n",
      "49361 Exodus 36:8 conj x p\n",
      "49364 Exodus 36:8 subs_a x p\n",
      "49365 Exodus 36:8 conj x p\n",
      "49375 Exodus 36:9 subs_a x p\n",
      "49378 Exodus 36:9 subs_a x p\n",
      "49379 Exodus 36:9 conj x p\n",
      "49385 Exodus 36:9 subs_a p x\n",
      "49389 Exodus 36:9 subs_a p x\n",
      "49423 Exodus 36:11 subs_a x p\n",
      "49449 Exodus 36:12 subs_a x p\n",
      "49474 Exodus 36:13 subs_a x p\n",
      "49503 Exodus 36:14 subs_a x p\n",
      "49509 Exodus 36:15 subs_a x p\n",
      "49547 Exodus 36:17 subs_a x p\n",
      "49573 Exodus 36:18 subs_a x p\n",
      "49590 Exodus 36:19 subs_a x p\n",
      "49594 Exodus 36:19 subs_a x p\n",
      "49616 Exodus 36:21 subs_a x p\n",
      "49617 Exodus 36:21 conj x p\n",
      "49623 Exodus 36:21 subs_a x p\n",
      "49630 Exodus 36:22 subs_a x p\n",
      "49659 Exodus 36:24 subs_a x p\n",
      "49664 Exodus 36:24 subs_a x p\n",
      "49682 Exodus 36:24 subs_a x p\n",
      "49699 Exodus 36:25 subs_a x p\n",
      "49702 Exodus 36:26 subs_a x p\n",
      "49703 Exodus 36:26 prps x p\n",
      "49727 Exodus 36:27 subs_a x p\n",
      "49734 Exodus 36:28 verb_c x p\n",
      "49754 Exodus 36:29 subs_a x p\n",
      "49767 Exodus 36:30 subs_a x p\n",
      "49768 Exodus 36:30 subs_a x p\n",
      "49769 Exodus 36:30 conj x p\n",
      "49770 Exodus 36:30 prps x p\n",
      "49771 Exodus 36:30 subs_a p x\n",
      "49776 Exodus 36:30 subs_a x p\n",
      "49794 Exodus 36:31 subs_a x p\n",
      "49870 Exodus 36:35 subs_a x p\n",
      "49880 Exodus 36:36 subs_a x p\n",
      "49891 Exodus 36:36 subs_a x p\n",
      "49908 Exodus 36:37 subs_a x p\n",
      "49914 Exodus 36:38 prps x p\n",
      "49915 Exodus 36:38 subs_a x p\n",
      "49916 Exodus 36:38 conj x p\n",
      "49926 Exodus 36:38 prps x p\n",
      "364006 Lamentations 2:2 subs_a x p\n",
      "364007 Lamentations 2:2 conj x p\n",
      "364150 Lamentations 2:9 subs_c p x\n",
      "364152 Lamentations 2:9 advb x p\n",
      "364198 Lamentations 2:11 subs_a x p\n",
      "364199 Lamentations 2:11 conj x p\n",
      "364217 Lamentations 2:12 subs_a x p\n",
      "364268 Lamentations 2:14 subs_a x p\n",
      "364269 Lamentations 2:14 conj x p\n",
      "364284 Lamentations 2:15 inrg p x\n",
      "364350 Lamentations 2:18 advb x p\n",
      "364351 Lamentations 2:18 conj x p\n",
      "364383 Lamentations 2:19 art p x\n",
      "364418 Lamentations 2:21 subs_a x p\n",
      "364419 Lamentations 2:21 conj x p\n",
      "364421 Lamentations 2:21 prps x p\n",
      "364422 Lamentations 2:21 conj x p\n",
      "349968 Proverbs 12:7 subs_c p x\n",
      "350027 Proverbs 12:13 subs_a p x\n",
      "350049 Proverbs 12:15 subs_a p x\n",
      "350072 Proverbs 12:18 subs_a x p\n",
      "350095 Proverbs 12:20 verb_c x p\n",
      "350155 Proverbs 12:27 subs_a x p\n",
      "87388 Numbers 28:3 subs_a x p\n",
      "87392 Numbers 28:4 subs_a x p\n",
      "87407 Numbers 28:4 subs_a p x\n",
      "87408 Numbers 28:5 conj p x\n",
      "87411 Numbers 28:5 subs_a x p\n",
      "87442 Numbers 28:7 subs_a x p\n",
      "87466 Numbers 28:8 subs_a x p\n",
      "87467 Numbers 28:8 conj x p\n",
      "87484 Numbers 28:9 subs_a p x\n",
      "87488 Numbers 28:9 subs_a x p\n",
      "87498 Numbers 28:10 subs_c x p\n",
      "87504 Numbers 28:10 subs_a x p\n",
      "87505 Numbers 28:10 conj x p\n",
      "87515 Numbers 28:11 subs_a x p\n",
      "87517 Numbers 28:11 subs_a x p\n",
      "87522 Numbers 28:11 subs_a x p\n",
      "87524 Numbers 28:11 subs_a x p\n",
      "87543 Numbers 28:12 subs_a x p\n",
      "87558 Numbers 28:13 subs_a p x\n",
      "87566 Numbers 28:13 subs_a x p\n",
      "87630 Numbers 28:16 subs_a x p\n",
      "87668 Numbers 28:19 subs_a x p\n",
      "87672 Numbers 28:19 subs_a x p\n",
      "87674 Numbers 28:19 subs_a x p\n",
      "87678 Numbers 28:19 subs_a p x\n",
      "87679 Numbers 28:19 conj p x\n",
      "87681 Numbers 28:19 subs_a x p\n",
      "87683 Numbers 28:19 subs_a p x\n",
      "87694 Numbers 28:20 subs_a x p\n",
      "87706 Numbers 28:21 subs_a x p\n",
      "87711 Numbers 28:21 subs_a x p\n",
      "87720 Numbers 28:22 subs_a x p\n",
      "87808 Numbers 28:27 subs_a p x\n",
      "87810 Numbers 28:27 subs_a p x\n",
      "87812 Numbers 28:27 subs_a p x\n",
      "87848 Numbers 28:30 subs_a x p\n",
      "87858 Numbers 28:31 subs_a x p\n",
      "87859 Numbers 28:31 conj x p\n",
      "220068 Isaiah 25:1 subs_a p x\n",
      "220140 Isaiah 25:6 nmpr_a x p\n",
      "220155 Isaiah 25:6 subs_a x p\n",
      "220157 Isaiah 25:6 subs_a x p\n",
      "220169 Isaiah 25:7 art x p\n",
      "220175 Isaiah 25:7 conj p x\n",
      "220191 Isaiah 25:8 nmpr_a x p\n",
      "280957 Ezekiel 33:8 prps x p\n",
      "281027 Ezekiel 33:11 conj x p\n",
      "281125 Ezekiel 33:14 subs_a x p\n",
      "281126 Ezekiel 33:14 conj x p\n",
      "281153 Ezekiel 33:16 subs_a x p\n",
      "281154 Ezekiel 33:16 conj x p\n",
      "281214 Ezekiel 33:21 subs_a x p\n",
      "281247 Ezekiel 33:22 verb_c p x\n",
      "281444 Ezekiel 33:30 subs_c p x\n",
      "281508 Ezekiel 33:32 subs_c p x\n",
      "317302 Psalms 41:8 subs_c x p\n",
      "317303 Psalms 41:8 verb_c x p\n",
      "317319 Psalms 41:10 advb x p\n",
      "317348 Psalms 41:12 verb_c x p\n",
      "317373 Psalms 41:14 intj x p\n",
      "317374 Psalms 41:14 conj x p\n",
      "300055 Micah 3:1 nmpr_a x p\n",
      "300056 Micah 3:1 conj x p\n",
      "300068 Micah 3:2 verb_c p x\n",
      "300071 Micah 3:2 verb_c p x\n",
      "300102 Micah 3:3 subs_a p x\n",
      "300103 Micah 3:3 conj p x\n",
      "300193 Micah 3:7 subs_c p x\n",
      "300197 Micah 3:8 advb p x\n",
      "300288 Micah 3:12 subs_a x p\n",
      "356259 Ruth 2:2 subs_c x p\n",
      "356366 Ruth 2:7 advb x p\n",
      "356373 Ruth 2:7 verb_c p x\n",
      "356421 Ruth 2:9 verb_c p x\n",
      "356488 Ruth 2:11 subs_a x p\n",
      "356578 Ruth 2:15 advb x p\n",
      "356587 Ruth 2:16 advb x p\n",
      "356618 Ruth 2:17 subs_a x p\n",
      "356704 Ruth 2:20 verb_c x p\n",
      "356760 Ruth 2:23 subs_a x p\n",
      "338299 Job 9:4 subs_a x p\n",
      "338300 Job 9:4 conj x p\n",
      "338308 Job 9:5 art p x\n",
      "338348 Job 9:9 subs_a x p\n",
      "338351 Job 9:9 subs_a x p\n",
      "338352 Job 9:9 conj x p\n",
      "338407 Job 9:15 verb_c x p\n",
      "338435 Job 9:19 subs_a p x\n",
      "338464 Job 9:22 adjv_a x p\n",
      "338465 Job 9:22 conj x p\n",
      "338501 Job 9:26 subs_a x p\n",
      "338509 Job 9:27 verb_c p x\n",
      "338525 Job 9:29 inrg x p\n",
      "268105 Ezekiel 9:2 subs_a p x\n",
      "268106 Ezekiel 9:2 conj p x\n",
      "268153 Ezekiel 9:3 subs_a p x\n",
      "268154 Ezekiel 9:3 art p x\n",
      "268215 Ezekiel 9:6 adjv_a x p\n",
      "268226 Ezekiel 9:6 conj p x\n",
      "268309 Ezekiel 9:9 nmpr_a x p\n",
      "268310 Ezekiel 9:9 conj x p\n",
      "268314 Ezekiel 9:9 subs_a x p\n",
      "268334 Ezekiel 9:9 subs_c p x\n",
      "268353 Ezekiel 9:11 subs_a x p\n",
      "347451 Proverbs 3:2 subs_a x p\n",
      "347452 Proverbs 3:2 conj x p\n",
      "347476 Proverbs 3:4 subs_a x p\n",
      "347477 Proverbs 3:4 conj x p\n",
      "347531 Proverbs 3:10 subs_a p x\n",
      "347532 Proverbs 3:10 conj p x\n",
      "347558 Proverbs 3:13 subs_c p x\n",
      "347610 Proverbs 3:18 verb_c p x\n",
      "347770 Proverbs 3:35 subs_a p x\n",
      "397744 1 Chronicles 12:1 verb_c x p\n",
      "397749 1 Chronicles 12:2 verb_a x p\n",
      "397750 1 Chronicles 12:2 conj x p\n",
      "397769 1 Chronicles 12:3 nmpr_a p x\n",
      "397770 1 Chronicles 12:3 conj p x\n",
      "397861 1 Chronicles 12:9 verb_c p x\n",
      "397919 1 Chronicles 12:15 subs_a p x\n",
      "397921 1 Chronicles 12:15 adjv_a p x\n",
      "397922 1 Chronicles 12:15 conj p x\n",
      "398000 1 Chronicles 12:18 prep p x\n",
      "398001 1 Chronicles 12:18 nega p x\n",
      "398030 1 Chronicles 12:19 verb_c x p\n",
      "398141 1 Chronicles 12:24 subs_c x p\n",
      "398142 1 Chronicles 12:24 art x p\n",
      "398161 1 Chronicles 12:25 verb_c p x\n",
      "398169 1 Chronicles 12:25 subs_a x p\n",
      "398170 1 Chronicles 12:25 verb_c x p\n",
      "398291 1 Chronicles 12:34 verb_c p x\n",
      "398304 1 Chronicles 12:34 prep p x\n",
      "398305 1 Chronicles 12:34 nega p x\n",
      "398317 1 Chronicles 12:35 subs_a x p\n",
      "398318 1 Chronicles 12:35 conj x p\n",
      "398319 1 Chronicles 12:35 subs_a p x\n",
      "398328 1 Chronicles 12:36 verb_c p x\n",
      "398358 1 Chronicles 12:38 adjv_a x p\n",
      "398359 1 Chronicles 12:38 conj x p\n",
      "398391 1 Chronicles 12:39 advb x p\n",
      "398417 1 Chronicles 12:41 art p x\n",
      "348039 Proverbs 5:2 subs_a p x\n",
      "348040 Proverbs 5:2 conj p x\n",
      "348059 Proverbs 5:4 subs_a p x\n",
      "348143 Proverbs 5:13 verb_c x p\n",
      "348155 Proverbs 5:14 subs_a x p\n",
      "348156 Proverbs 5:14 conj x p\n",
      "348193 Proverbs 5:19 subs_a x p\n",
      "348194 Proverbs 5:19 conj x p\n",
      "395594 1 Chronicles 8:3 nmpr_a p x\n",
      "395595 1 Chronicles 8:4 conj p x\n",
      "395600 1 Chronicles 8:4 nmpr_a p x\n",
      "395601 1 Chronicles 8:5 conj p x\n",
      "395616 1 Chronicles 8:6 verb_c x p\n",
      "395621 1 Chronicles 8:6 nmpr_a p x\n",
      "395622 1 Chronicles 8:7 conj p x\n",
      "395666 1 Chronicles 8:9 nmpr_a p x\n",
      "395667 1 Chronicles 8:10 conj p x\n",
      "395703 1 Chronicles 8:12 nmpr_a x p\n",
      "395704 1 Chronicles 8:12 conj x p\n",
      "395715 1 Chronicles 8:13 verb_c x p\n",
      "395720 1 Chronicles 8:13 verb_c x p\n",
      "395721 1 Chronicles 8:13 nmpr_a p x\n",
      "395722 1 Chronicles 8:14 conj p x\n",
      "395726 1 Chronicles 8:14 nmpr_a p x\n",
      "395727 1 Chronicles 8:15 conj p x\n",
      "395732 1 Chronicles 8:15 nmpr_a p x\n",
      "395733 1 Chronicles 8:16 conj p x\n",
      "395748 1 Chronicles 8:17 nmpr_a p x\n",
      "395749 1 Chronicles 8:18 conj p x\n",
      "395762 1 Chronicles 8:19 nmpr_a p x\n",
      "395763 1 Chronicles 8:20 conj p x\n",
      "395768 1 Chronicles 8:20 nmpr_a p x\n",
      "395769 1 Chronicles 8:21 conj p x\n",
      "395782 1 Chronicles 8:22 nmpr_a p x\n",
      "395783 1 Chronicles 8:23 conj p x\n",
      "395788 1 Chronicles 8:23 nmpr_a p x\n",
      "395789 1 Chronicles 8:24 conj p x\n",
      "395794 1 Chronicles 8:24 nmpr_a p x\n",
      "395795 1 Chronicles 8:25 conj p x\n",
      "395806 1 Chronicles 8:26 nmpr_a p x\n",
      "395807 1 Chronicles 8:27 conj p x\n",
      "395839 1 Chronicles 8:30 nmpr_a p x\n",
      "395840 1 Chronicles 8:30 conj p x\n",
      "395847 1 Chronicles 8:30 nmpr_a p x\n",
      "395848 1 Chronicles 8:31 conj p x\n",
      "395853 1 Chronicles 8:31 nmpr_a p x\n",
      "395854 1 Chronicles 8:32 conj p x\n",
      "395911 1 Chronicles 8:35 nmpr_a p x\n",
      "395912 1 Chronicles 8:36 conj p x\n",
      "395990 1 Chronicles 8:40 subs_a x p\n",
      "395991 1 Chronicles 8:40 conj x p\n",
      "296538 Amos 5:2 subs_c p x\n",
      "296539 Amos 5:2 verb_c p x\n",
      "296553 Amos 5:3 art p x\n",
      "296625 Amos 5:8 verb_c p x\n",
      "296636 Amos 5:8 subs_a p x\n",
      "296673 Amos 5:11 subs_c x p\n",
      "296704 Amos 5:12 verb_c p x\n",
      "296715 Amos 5:13 art x p\n",
      "296778 Amos 5:16 intj x p\n",
      "296788 Amos 5:16 verb_c x p\n",
      "296793 Amos 5:17 subs_a p x\n",
      "296850 Amos 5:20 subs_a p x\n",
      "296851 Amos 5:20 conj p x\n",
      "296869 Amos 5:22 subs_a x p\n",
      "296870 Amos 5:22 conj x p\n",
      "296901 Amos 5:25 subs_a x p\n",
      "296902 Amos 5:25 conj x p\n",
      "188449 1 Kings 15:5 advb x p\n",
      "188539 1 Kings 15:11 art p x\n",
      "188593 1 Kings 15:14 advb x p\n",
      "188606 1 Kings 15:15 prps x p\n",
      "188607 1 Kings 15:15 conj x p\n",
      "188615 1 Kings 15:15 subs_a p x\n",
      "188616 1 Kings 15:16 conj p x\n",
      "188643 1 Kings 15:17 verb_a x p\n",
      "188644 1 Kings 15:17 conj x p\n",
      "188697 1 Kings 15:19 prps x p\n",
      "188701 1 Kings 15:19 prps x p\n",
      "188702 1 Kings 15:19 conj x p\n",
      "188709 1 Kings 15:19 subs_a x p\n",
      "188710 1 Kings 15:19 conj x p\n",
      "188784 1 Kings 15:22 subs_c p x\n",
      "188805 1 Kings 15:22 nmpr_a x p\n",
      "188806 1 Kings 15:22 nmpr_a x p\n",
      "188807 1 Kings 15:22 conj x p\n",
      "331331 Psalms 119:2 subs_c p x\n",
      "331470 Psalms 119:21 adjv_a x p\n",
      "331494 Psalms 119:24 advb x p\n",
      "331519 Psalms 119:27 verb_c x p\n",
      "331596 Psalms 119:38 verb_c x p\n",
      "331644 Psalms 119:44 subs_a x p\n",
      "331645 Psalms 119:44 conj x p\n",
      "331789 Psalms 119:63 verb_c x p\n",
      "331830 Psalms 119:69 subs_a p x\n",
      "331859 Psalms 119:72 subs_a x p\n",
      "331860 Psalms 119:72 conj x p\n",
      "331915 Psalms 119:79 verb_c p x\n",
      "331956 Psalms 119:84 verb_c x p\n",
      "331999 Psalms 119:90 subs_a x p\n",
      "332000 Psalms 119:90 conj x p\n",
      "332015 Psalms 119:91 prps p x\n",
      "332016 Psalms 119:92 conj p x\n",
      "332048 Psalms 119:96 subs_a p x\n",
      "332061 Psalms 119:98 verb_c x p\n",
      "332071 Psalms 119:99 verb_c x p\n",
      "332179 Psalms 119:112 subs_a p x\n",
      "332218 Psalms 119:118 subs_c p x\n",
      "332242 Psalms 119:121 subs_a x p\n",
      "332243 Psalms 119:121 conj x p\n",
      "332248 Psalms 119:121 verb_c x p\n",
      "332259 Psalms 119:123 prps x p\n",
      "332260 Psalms 119:123 conj x p\n",
      "332330 Psalms 119:132 verb_c x p\n",
      "332373 Psalms 119:138 prps x p\n",
      "332374 Psalms 119:138 conj x p\n",
      "332375 Psalms 119:138 subs_a p x\n",
      "332402 Psalms 119:142 subs_a p x\n",
      "332403 Psalms 119:143 adjv_a x p\n",
      "332404 Psalms 119:143 conj x p\n",
      "332454 Psalms 119:150 verb_c x p\n",
      "332504 Psalms 119:157 verb_c x p\n",
      "332504 Psalms 119:157 prps x p\n",
      "332505 Psalms 119:157 conj x p\n",
      "332569 Psalms 119:165 verb_c x p\n",
      "332589 Psalms 119:168 prps x p\n",
      "332591 Psalms 119:168 prps p x\n",
      "332592 Psalms 119:168 conj p x\n",
      "332647 Psalms 119:176 subs_a x p\n",
      "413500 2 Chronicles 14:1 art p x\n",
      "413501 2 Chronicles 14:1 adjv_a p x\n",
      "413502 2 Chronicles 14:1 conj p x\n",
      "413503 2 Chronicles 14:1 art p x\n",
      "413542 2 Chronicles 14:3 subs_a x p\n",
      "413606 2 Chronicles 14:6 subs_c p x\n",
      "413644 2 Chronicles 14:7 subs_a x p\n",
      "413645 2 Chronicles 14:7 conj x p\n",
      "413663 2 Chronicles 14:8 subs_a x p\n",
      "413735 2 Chronicles 14:11 nmpr_a x p\n",
      "413736 2 Chronicles 14:11 conj x p\n",
      "413746 2 Chronicles 14:12 nmpr_a x p\n",
      "413747 2 Chronicles 14:12 conj x p\n",
      "413774 2 Chronicles 14:12 subs_a x p\n",
      "413775 2 Chronicles 14:12 verb_a x p\n",
      "413802 2 Chronicles 14:14 advb x p\n"
     ]
    }
   ],
   "source": [
    "for error in bible_section:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17252 1QS 2:2 subs_a x p\n",
      "17276 1QS 2:3 intj x p\n",
      "17308 1QS 2:5 prps p x\n",
      "17309 1QS 2:5 conj p x\n",
      "17331 1QS 2:6 prps x p\n",
      "17332 1QS 2:6 conj x p\n",
      "17450 1QS 2:12 subs_c p x\n",
      "17451 1QS 2:12 verb_c p x\n",
      "17460 1QS 2:12 subs_c p x\n",
      "17461 1QS 2:12 verb_c p x\n",
      "17465 1QS 2:13 prep p x\n",
      "17466 1QS 2:13 subs_c p x\n",
      "17508 1QS 2:15 subs_c p x\n",
      "17509 1QS 2:15 verb_c p x\n",
      "17525 1QS 2:16 intj x p\n",
      "17594 1QS 2:18 adjv_a x p\n",
      "17598 1QS 2:18 prep p x\n",
      "17599 1QS 2:18 subs_c p x\n",
      "17602 1QS 2:19 subs_a x p\n",
      "17603 1QS 2:19 conj x p\n",
      "17643 1QS 2:20 prps x p\n",
      "17644 1QS 2:20 conj x p\n",
      "17656 1QS 2:22 subs_c p x\n",
      "17657 1QS 2:22 verb_c p x\n",
      "17665 1QS 2:22 intj x p\n",
      "17670 1QS 2:23 subs_a x p\n",
      "17690 1QS 2:24 prde x p\n",
      "17710 1QS 2:25 prde x p\n",
      "17757 1QS 2:27 subs_a x p\n",
      "17758 1QS 2:27 conj x p\n",
      "17891 1QS 2:36 subs_c p x\n",
      "17940 1QS 2:38 subs_a x p\n",
      "17941 1QS 2:38 conj x p\n",
      "18918 1QS 4:1 verb_c p x\n",
      "18957 1QS 4:2 subs_a p x\n",
      "18958 1QS 4:2 conj p x\n",
      "18993 1QS 4:3 conj x p\n",
      "19047 1QS 4:3 subs_c p x\n",
      "19048 1QS 4:3 verb_c p x\n",
      "19118 1QS 4:6 verb_c p x\n",
      "19122 1QS 4:6 verb_c p x\n",
      "19125 1QS 4:6 conj p x\n",
      "19183 1QS 4:8 art x p\n",
      "19196 1QS 4:9 subs_a x p\n",
      "19220 1QS 4:9 prep p x\n",
      "19221 1QS 4:9 subs_c p x\n",
      "19238 1QS 4:11 conj x p\n",
      "19247 1QS 4:11 subs_c p x\n",
      "19248 1QS 4:11 verb_c p x\n",
      "19381 1QS 4:17 subs_c p x\n",
      "19382 1QS 4:17 verb_c p x\n",
      "19438 1QS 4:18 prps x p\n",
      "19439 1QS 4:18 conj x p\n",
      "19441 1QS 4:18 prps x p\n",
      "19499 1QS 4:19 prps x p\n",
      "19500 1QS 4:19 conj x p\n",
      "19510 1QS 4:19 prps p x\n",
      "19522 1QS 4:20 prps x p\n",
      "19523 1QS 4:20 conj x p\n",
      "19526 1QS 4:20 subs_a x p\n",
      "19535 1QS 4:20 prps x p\n",
      "19545 1QS 4:20 verb_c x p\n",
      "19556 1QS 4:21 subs_a x p\n",
      "19557 1QS 4:21 conj x p\n",
      "19574 1QS 4:22 adjv_a x p\n",
      "19575 1QS 4:22 conj x p\n",
      "19626 1QS 4:26 subs_c p x\n",
      "19661 1QS 4:29 subs_a x p\n",
      "19662 1QS 4:29 subs_a x p\n",
      "19672 1QS 4:29 subs_a x p\n",
      "19757 1QS 4:32 advb x p\n",
      "19758 1QS 4:32 conj x p\n",
      "19759 1QS 4:32 subs_a p x\n",
      "19760 1QS 4:32 subs_a p x\n",
      "19791 1QS 4:33 subs_a p x\n",
      "19809 1QS 4:35 adjv_a p x\n",
      "19810 1QS 4:35 conj p x\n",
      "19881 1QS 4:39 subs_a x p\n",
      "19882 1QS 4:39 art x p\n",
      "19915 1QS 4:41 subs_a p x\n",
      "19916 1QS 4:41 conj p x\n",
      "19976 1QS 4:42 subs_a x p\n",
      "19978 1QS 4:42 subs_a x p\n",
      "20010 1QS 4:43 verb_c p x\n",
      "20068 1QS 4:46 prps x p\n",
      "20069 1QS 4:46 conj x p\n",
      "20079 1QS 4:47 advb p x\n",
      "20106 1QS 4:48 prps x p\n",
      "20107 1QS 4:48 conj x p\n",
      "20109 1QS 4:48 prps x p\n",
      "20129 1QS 4:49 subs_a x p\n",
      "20130 1QS 4:49 conj x p\n",
      "22055 1QS 7:5 prps x p\n",
      "22059 1QS 7:6 subs_a x p\n",
      "22066 1QS 7:6 subs_a x p\n",
      "22069 1QS 7:6 prps x p\n",
      "22070 1QS 7:6 conj x p\n",
      "22072 1QS 7:6 subs_a x p\n",
      "22077 1QS 7:6 subs_a x p\n",
      "22093 1QS 7:7 subs_a x p\n",
      "22094 1QS 7:7 conj x p\n",
      "22107 1QS 7:7 prde x p\n",
      "22125 1QS 7:8 prps p x\n",
      "22126 1QS 7:8 conj p x\n",
      "22136 1QS 7:9 subs_c p x\n",
      "22137 1QS 7:9 verb_c p x\n",
      "22146 1QS 7:9 subs_a x p\n",
      "22147 1QS 7:9 conj x p\n",
      "22180 1QS 7:11 subs_a x p\n",
      "22181 1QS 7:11 conj x p\n",
      "22190 1QS 7:11 subs_a x p\n",
      "22191 1QS 7:11 conj x p\n",
      "22211 1QS 7:13 verb_c x p\n",
      "22212 1QS 7:13 prps x p\n",
      "22213 1QS 7:13 conj x p\n",
      "22221 1QS 7:13 subs_a x p\n",
      "22236 1QS 7:14 subs_a x p\n",
      "22237 1QS 7:14 conj x p\n",
      "22241 1QS 7:14 subs_a x p\n",
      "22242 1QS 7:14 conj x p\n",
      "22244 1QS 7:14 subs_a x p\n",
      "22263 1QS 7:16 prps x p\n",
      "22264 1QS 7:16 conj x p\n",
      "22271 1QS 7:16 subs_c p x\n",
      "22313 1QS 7:19 subs_a x p\n",
      "22314 1QS 7:19 conj x p\n",
      "22317 1QS 7:19 subs_a x p\n",
      "22436 1QS 7:27 subs_c p x\n",
      "22437 1QS 7:27 verb_c p x\n",
      "22509 1QS 7:31 subs_a x p\n",
      "22510 1QS 7:31 conj x p\n",
      "22527 1QS 7:32 subs_a x p\n",
      "22531 1QS 7:32 subs_a x p\n",
      "22532 1QS 7:32 conj x p\n",
      "22544 1QS 7:33 subs_a x p\n",
      "22573 1QS 7:33 subs_c p x\n",
      "22574 1QS 7:33 verb_c p x\n",
      "22578 1QS 7:33 subs_a x p\n",
      "22583 1QS 7:33 verb_c p x\n",
      "22586 1QS 7:33 verb_c p x\n",
      "22589 1QS 7:33 verb_c p x\n",
      "22603 1QS 7:34 prps x p\n",
      "22625 1QS 7:35 verb_c x p\n",
      "22636 1QS 7:35 verb_a x p\n",
      "22637 1QS 7:35 conj x p\n",
      "22660 1QS 7:37 prps p x\n",
      "22661 1QS 7:37 conj p x\n",
      "22674 1QS 7:38 subs_a x p\n",
      "22683 1QS 7:39 verb_c x p\n",
      "22693 1QS 7:39 subs_a x p\n",
      "22694 1QS 7:39 conj x p\n",
      "22701 1QS 7:39 subs_a x p\n",
      "22702 1QS 7:39 conj x p\n",
      "22704 1QS 7:39 subs_a x p\n",
      "22735 1QS 7:40 subs_a x p\n",
      "22736 1QS 7:40 conj x p\n",
      "22758 1QS 7:42 prps x p\n",
      "22760 1QS 7:42 prps x p\n",
      "22762 1QS 7:42 prps x p\n",
      "22764 1QS 7:42 verb_c x p\n",
      "22769 1QS 7:42 subs_a p x\n",
      "22770 1QS 7:42 conj p x\n",
      "22771 1QS 7:42 verb_c p x\n",
      "22787 1QS 7:44 subs_a p x\n",
      "22804 1QS 7:45 subs_c x p\n",
      "22896 1QS 7:50 subs_a x p\n",
      "22897 1QS 7:50 conj x p\n",
      "22968 1QS 7:54 subs_c x p\n",
      "22969 1QS 7:54 art x p\n",
      "22976 1QS 7:55 subs_c p x\n",
      "23005 1QS 7:55 verb_c x p\n",
      "23006 1QS 7:55 prps x p\n",
      "23051 1QS 7:60 subs_a p x\n",
      "23052 1QS 7:60 subs_a x p\n",
      "23062 1QS 7:61 subs_a x p\n",
      "23063 1QS 7:61 conj x p\n",
      "16978 1QS 1:2 subs_a x p\n",
      "16979 1QS 1:2 conj x p\n",
      "16998 1QS 1:2 nmpr_a p x\n",
      "16999 1QS 1:2 conj p x\n",
      "17051 1QS 1:6 subs_a x p\n",
      "17052 1QS 1:6 conj x p\n",
      "17101 1QS 1:9 prps x p\n",
      "17114 1QS 1:9 prps x p\n",
      "17201 1QS 1:12 subs_a x p\n",
      "17202 1QS 1:12 conj x p\n",
      "20894 1QS 6:1 subs_a x p\n",
      "20895 1QS 6:1 conj x p\n",
      "20896 1QS 6:1 subs_a x p\n",
      "20897 1QS 6:1 subs_a p x\n",
      "20900 1QS 6:1 subs_c p x\n",
      "20913 1QS 6:1 subs_a x p\n",
      "20914 1QS 6:1 conj x p\n",
      "20931 1QS 6:1 subs_a x p\n",
      "20932 1QS 6:1 verb_a x p\n",
      "20933 1QS 6:1 conj x p\n",
      "20934 1QS 6:1 subs_a x p\n",
      "20942 1QS 6:1 subs_a x p\n",
      "20943 1QS 6:1 conj x p\n",
      "21040 1QS 6:3 adjv_a x p\n",
      "21041 1QS 6:3 conj x p\n",
      "21064 1QS 6:4 subs_a p x\n",
      "21065 1QS 6:4 conj p x\n",
      "21066 1QS 6:4 subs_c p x\n",
      "21099 1QS 6:5 subs_a x p\n",
      "21172 1QS 6:7 subs_c p x\n",
      "21173 1QS 6:7 art p x\n",
      "21207 1QS 6:8 subs_a x p\n",
      "21259 1QS 6:10 subs_c p x\n",
      "21275 1QS 6:11 adjv_a x p\n",
      "21276 1QS 6:11 subs_a p x\n",
      "21375 1QS 6:15 subs_a x p\n",
      "21393 1QS 6:15 subs_a x p\n",
      "21397 1QS 6:16 subs_a x p\n",
      "21403 1QS 6:16 art p x\n",
      "21406 1QS 6:16 subs_a x p\n",
      "21411 1QS 6:17 advb x p\n",
      "21415 1QS 6:17 subs_a x p\n",
      "21420 1QS 6:17 prps x p\n",
      "21421 1QS 6:17 conj x p\n",
      "21463 1QS 6:19 subs_a p x\n",
      "21464 1QS 6:19 conj p x\n",
      "21472 1QS 6:19 subs_a x p\n",
      "21473 1QS 6:19 conj x p\n",
      "21485 1QS 6:20 subs_a x p\n",
      "21509 1QS 6:21 subs_a x p\n",
      "21510 1QS 6:21 conj x p\n",
      "21519 1QS 6:22 advb x p\n",
      "21610 1QS 6:26 subs_a x p\n",
      "21611 1QS 6:26 conj x p\n",
      "21633 1QS 6:27 subs_a x p\n",
      "21634 1QS 6:27 conj x p\n",
      "21759 1QS 6:31 subs_a x p\n",
      "21760 1QS 6:31 conj x p\n",
      "21808 1QS 6:32 subs_c p x\n",
      "21867 1QS 6:35 subs_a x p\n",
      "21873 1QS 6:35 subs_a x p\n",
      "21874 1QS 6:35 conj x p\n",
      "21880 1QS 6:35 art p x\n",
      "21912 1QS 6:36 subs_a x p\n",
      "21913 1QS 6:36 conj x p\n",
      "21959 1QS 6:38 verb_c p x\n",
      "20894 1QS 6:1 subs_a x p\n",
      "20895 1QS 6:1 conj x p\n",
      "20896 1QS 6:1 subs_a x p\n",
      "20897 1QS 6:1 subs_a p x\n",
      "20900 1QS 6:1 subs_c p x\n",
      "20913 1QS 6:1 subs_a x p\n",
      "20914 1QS 6:1 conj x p\n",
      "20931 1QS 6:1 subs_a x p\n",
      "20932 1QS 6:1 verb_a x p\n",
      "20933 1QS 6:1 conj x p\n",
      "20934 1QS 6:1 subs_a x p\n",
      "20942 1QS 6:1 subs_a x p\n",
      "20943 1QS 6:1 conj x p\n",
      "21040 1QS 6:3 adjv_a x p\n",
      "21041 1QS 6:3 conj x p\n",
      "21064 1QS 6:4 subs_a p x\n",
      "21065 1QS 6:4 conj p x\n",
      "21066 1QS 6:4 subs_c p x\n",
      "21099 1QS 6:5 subs_a x p\n",
      "21172 1QS 6:7 subs_c p x\n",
      "21173 1QS 6:7 art p x\n",
      "21207 1QS 6:8 subs_a x p\n",
      "21259 1QS 6:10 subs_c p x\n",
      "21275 1QS 6:11 adjv_a x p\n",
      "21276 1QS 6:11 subs_a p x\n",
      "21375 1QS 6:15 subs_a x p\n",
      "21393 1QS 6:15 subs_a x p\n",
      "21397 1QS 6:16 subs_a x p\n",
      "21403 1QS 6:16 art p x\n",
      "21406 1QS 6:16 subs_a x p\n",
      "21411 1QS 6:17 advb x p\n",
      "21415 1QS 6:17 subs_a x p\n",
      "21420 1QS 6:17 prps x p\n",
      "21421 1QS 6:17 conj x p\n",
      "21463 1QS 6:19 subs_a p x\n",
      "21464 1QS 6:19 conj p x\n",
      "21472 1QS 6:19 subs_a x p\n",
      "21473 1QS 6:19 conj x p\n",
      "21485 1QS 6:20 subs_a x p\n",
      "21509 1QS 6:21 subs_a x p\n",
      "21510 1QS 6:21 conj x p\n",
      "21519 1QS 6:22 advb x p\n",
      "21610 1QS 6:26 subs_a x p\n",
      "21611 1QS 6:26 conj x p\n",
      "21633 1QS 6:27 subs_a x p\n",
      "21634 1QS 6:27 conj x p\n",
      "21759 1QS 6:31 subs_a x p\n",
      "21760 1QS 6:31 conj x p\n",
      "21808 1QS 6:32 subs_c p x\n",
      "21867 1QS 6:35 subs_a x p\n",
      "21873 1QS 6:35 subs_a x p\n",
      "21874 1QS 6:35 conj x p\n",
      "21880 1QS 6:35 art p x\n",
      "21912 1QS 6:36 subs_a x p\n",
      "21913 1QS 6:36 conj x p\n",
      "21959 1QS 6:38 verb_c p x\n",
      "18049 1QS 3:1 prps p x\n",
      "18050 1QS 3:1 conj p x\n",
      "18064 1QS 3:2 verb_a x p\n",
      "18065 1QS 3:2 conj x p\n",
      "18070 1QS 3:3 verb_c p x\n",
      "18182 1QS 3:11 conj p x\n",
      "18334 1QS 3:19 subs_a x p\n",
      "18335 1QS 3:19 conj x p\n",
      "18351 1QS 3:19 subs_a x p\n",
      "18355 1QS 3:19 subs_a x p\n",
      "18356 1QS 3:19 conj x p\n",
      "18359 1QS 3:19 subs_a x p\n",
      "18360 1QS 3:19 conj x p\n",
      "18362 1QS 3:19 subs_a x p\n",
      "18364 1QS 3:19 subs_a x p\n",
      "18365 1QS 3:19 verb_a x p\n",
      "18366 1QS 3:19 conj x p\n",
      "18368 1QS 3:19 subs_a x p\n",
      "18372 1QS 3:19 subs_a x p\n",
      "18373 1QS 3:19 conj x p\n",
      "18406 1QS 3:20 subs_a x p\n",
      "18407 1QS 3:20 conj x p\n",
      "18419 1QS 3:20 subs_a x p\n",
      "18420 1QS 3:20 conj x p\n",
      "18447 1QS 3:21 subs_a x p\n",
      "18450 1QS 3:21 subs_a x p\n",
      "18451 1QS 3:21 conj x p\n",
      "18453 1QS 3:21 subs_a p x\n",
      "18457 1QS 3:21 adjv_a x p\n",
      "18458 1QS 3:21 conj x p\n",
      "18470 1QS 3:21 subs_a x p\n",
      "18473 1QS 3:21 subs_a x p\n",
      "18474 1QS 3:21 conj x p\n",
      "18476 1QS 3:21 subs_a x p\n",
      "18482 1QS 3:21 subs_a x p\n",
      "18484 1QS 3:21 subs_a x p\n",
      "18485 1QS 3:21 conj x p\n",
      "18487 1QS 3:21 subs_a x p\n",
      "18489 1QS 3:21 subs_a x p\n",
      "18490 1QS 3:21 conj x p\n",
      "18498 1QS 3:21 subs_a x p\n",
      "18499 1QS 3:21 conj x p\n",
      "18501 1QS 3:21 subs_a p x\n",
      "18502 1QS 3:22 conj p x\n",
      "18522 1QS 3:22 subs_a x p\n",
      "18526 1QS 3:22 subs_a x p\n",
      "18527 1QS 3:22 conj x p\n",
      "18545 1QS 3:23 subs_a x p\n",
      "18546 1QS 3:23 conj x p\n",
      "18555 1QS 3:23 prep p x\n",
      "18556 1QS 3:23 subs_c p x\n",
      "18557 1QS 3:23 subs_a x p\n",
      "18558 1QS 3:23 conj x p\n",
      "18608 1QS 3:26 subs_a x p\n",
      "18647 1QS 3:29 prps x p\n",
      "18648 1QS 3:29 conj x p\n",
      "18671 1QS 3:30 subs_a p x\n",
      "18745 1QS 3:32 adjv_a x p\n",
      "18746 1QS 3:32 conj x p\n",
      "18769 1QS 3:34 subs_c p x\n",
      "18781 1QS 3:35 subs_a x p\n",
      "18782 1QS 3:35 conj x p\n",
      "18789 1QS 3:36 subs_a x p\n",
      "18790 1QS 3:36 conj x p\n",
      "18798 1QS 3:36 subs_a x p\n",
      "18799 1QS 3:36 conj x p\n",
      "18820 1QS 3:37 subs_a x p\n",
      "18849 1QS 3:39 adjv_a x p\n",
      "18850 1QS 3:39 conj x p\n",
      "20330 1QS 5:3 subs_a x p\n",
      "20339 1QS 5:4 subs_c x p\n",
      "20340 1QS 5:4 art x p\n",
      "20343 1QS 5:4 subs_c x p\n",
      "20352 1QS 5:4 subs_a x p\n",
      "20353 1QS 5:4 conj x p\n",
      "20395 1QS 5:5 subs_a x p\n",
      "20412 1QS 5:6 subs_a x p\n",
      "20422 1QS 5:6 subs_a x p\n",
      "20438 1QS 5:6 subs_a x p\n",
      "20458 1QS 5:6 subs_a x p\n",
      "20468 1QS 5:7 subs_a x p\n",
      "20497 1QS 5:9 subs_a x p\n",
      "20511 1QS 5:10 subs_a x p\n",
      "20534 1QS 5:12 art p x\n",
      "20555 1QS 5:13 advb p x\n",
      "20570 1QS 5:13 advb p x\n",
      "20575 1QS 5:13 subs_a x p\n",
      "20588 1QS 5:14 subs_a x p\n",
      "20604 1QS 5:15 subs_a x p\n",
      "20605 1QS 5:15 subs_a p x\n",
      "20606 1QS 5:16 conj p x\n",
      "20617 1QS 5:16 subs_a x p\n",
      "20637 1QS 5:17 subs_a x p\n",
      "20650 1QS 5:18 subs_a x p\n",
      "20679 1QS 5:20 subs_a x p\n",
      "20730 1QS 5:23 subs_a x p\n",
      "20790 1QS 5:26 subs_a x p\n",
      "20854 1QS 5:28 subs_a p x\n",
      "20855 1QS 5:29 conj p x\n",
      "20330 1QS 5:3 subs_a x p\n",
      "20339 1QS 5:4 subs_c x p\n",
      "20340 1QS 5:4 art x p\n",
      "20343 1QS 5:4 subs_c x p\n",
      "20352 1QS 5:4 subs_a x p\n",
      "20353 1QS 5:4 conj x p\n",
      "20395 1QS 5:5 subs_a x p\n",
      "20412 1QS 5:6 subs_a x p\n",
      "20422 1QS 5:6 subs_a x p\n",
      "20438 1QS 5:6 subs_a x p\n",
      "20458 1QS 5:6 subs_a x p\n",
      "20468 1QS 5:7 subs_a x p\n",
      "20497 1QS 5:9 subs_a x p\n",
      "20511 1QS 5:10 subs_a x p\n",
      "20534 1QS 5:12 art p x\n",
      "20555 1QS 5:13 advb p x\n",
      "20570 1QS 5:13 advb p x\n",
      "20575 1QS 5:13 subs_a x p\n",
      "20588 1QS 5:14 subs_a x p\n",
      "20604 1QS 5:15 subs_a x p\n",
      "20605 1QS 5:15 subs_a p x\n",
      "20606 1QS 5:16 conj p x\n",
      "20617 1QS 5:16 subs_a x p\n",
      "20637 1QS 5:17 subs_a x p\n",
      "20650 1QS 5:18 subs_a x p\n",
      "20679 1QS 5:20 subs_a x p\n",
      "20730 1QS 5:23 subs_a x p\n",
      "20790 1QS 5:26 subs_a x p\n",
      "20854 1QS 5:28 subs_a p x\n",
      "20855 1QS 5:29 conj p x\n",
      "18918 1QS 4:1 verb_c p x\n",
      "18957 1QS 4:2 subs_a p x\n",
      "18958 1QS 4:2 conj p x\n",
      "18993 1QS 4:3 conj x p\n",
      "19047 1QS 4:3 subs_c p x\n",
      "19048 1QS 4:3 verb_c p x\n",
      "19118 1QS 4:6 verb_c p x\n",
      "19122 1QS 4:6 verb_c p x\n",
      "19125 1QS 4:6 conj p x\n",
      "19183 1QS 4:8 art x p\n",
      "19196 1QS 4:9 subs_a x p\n",
      "19220 1QS 4:9 prep p x\n",
      "19221 1QS 4:9 subs_c p x\n",
      "19238 1QS 4:11 conj x p\n",
      "19247 1QS 4:11 subs_c p x\n",
      "19248 1QS 4:11 verb_c p x\n",
      "19381 1QS 4:17 subs_c p x\n",
      "19382 1QS 4:17 verb_c p x\n",
      "19438 1QS 4:18 prps x p\n",
      "19439 1QS 4:18 conj x p\n",
      "19441 1QS 4:18 prps x p\n",
      "19499 1QS 4:19 prps x p\n",
      "19500 1QS 4:19 conj x p\n",
      "19510 1QS 4:19 prps p x\n",
      "19522 1QS 4:20 prps x p\n",
      "19523 1QS 4:20 conj x p\n",
      "19526 1QS 4:20 subs_a x p\n",
      "19535 1QS 4:20 prps x p\n",
      "19545 1QS 4:20 verb_c x p\n",
      "19556 1QS 4:21 subs_a x p\n",
      "19557 1QS 4:21 conj x p\n",
      "19574 1QS 4:22 adjv_a x p\n",
      "19575 1QS 4:22 conj x p\n",
      "19626 1QS 4:26 subs_c p x\n",
      "19661 1QS 4:29 subs_a x p\n",
      "19662 1QS 4:29 subs_a x p\n",
      "19672 1QS 4:29 subs_a x p\n",
      "19757 1QS 4:32 advb x p\n",
      "19758 1QS 4:32 conj x p\n",
      "19759 1QS 4:32 subs_a p x\n",
      "19760 1QS 4:32 subs_a p x\n",
      "19791 1QS 4:33 subs_a p x\n",
      "19809 1QS 4:35 adjv_a p x\n",
      "19810 1QS 4:35 conj p x\n",
      "19881 1QS 4:39 subs_a x p\n",
      "19882 1QS 4:39 art x p\n",
      "19915 1QS 4:41 subs_a p x\n",
      "19916 1QS 4:41 conj p x\n",
      "19976 1QS 4:42 subs_a x p\n",
      "19978 1QS 4:42 subs_a x p\n",
      "20010 1QS 4:43 verb_c p x\n",
      "20068 1QS 4:46 prps x p\n",
      "20069 1QS 4:46 conj x p\n",
      "20079 1QS 4:47 advb p x\n",
      "20106 1QS 4:48 prps x p\n",
      "20107 1QS 4:48 conj x p\n",
      "20109 1QS 4:48 prps x p\n",
      "20129 1QS 4:49 subs_a x p\n",
      "20130 1QS 4:49 conj x p\n",
      "22055 1QS 7:5 prps x p\n",
      "22059 1QS 7:6 subs_a x p\n",
      "22066 1QS 7:6 subs_a x p\n",
      "22069 1QS 7:6 prps x p\n",
      "22070 1QS 7:6 conj x p\n",
      "22072 1QS 7:6 subs_a x p\n",
      "22077 1QS 7:6 subs_a x p\n",
      "22093 1QS 7:7 subs_a x p\n",
      "22094 1QS 7:7 conj x p\n",
      "22107 1QS 7:7 prde x p\n",
      "22125 1QS 7:8 prps p x\n",
      "22126 1QS 7:8 conj p x\n",
      "22136 1QS 7:9 subs_c p x\n",
      "22137 1QS 7:9 verb_c p x\n",
      "22146 1QS 7:9 subs_a x p\n",
      "22147 1QS 7:9 conj x p\n",
      "22180 1QS 7:11 subs_a x p\n",
      "22181 1QS 7:11 conj x p\n",
      "22190 1QS 7:11 subs_a x p\n",
      "22191 1QS 7:11 conj x p\n",
      "22211 1QS 7:13 verb_c x p\n",
      "22212 1QS 7:13 prps x p\n",
      "22213 1QS 7:13 conj x p\n",
      "22221 1QS 7:13 subs_a x p\n",
      "22236 1QS 7:14 subs_a x p\n",
      "22237 1QS 7:14 conj x p\n",
      "22241 1QS 7:14 subs_a x p\n",
      "22242 1QS 7:14 conj x p\n",
      "22244 1QS 7:14 subs_a x p\n",
      "22263 1QS 7:16 prps x p\n",
      "22264 1QS 7:16 conj x p\n",
      "22271 1QS 7:16 subs_c p x\n",
      "22313 1QS 7:19 subs_a x p\n",
      "22314 1QS 7:19 conj x p\n",
      "22317 1QS 7:19 subs_a x p\n",
      "22436 1QS 7:27 subs_c p x\n",
      "22437 1QS 7:27 verb_c p x\n",
      "22509 1QS 7:31 subs_a x p\n",
      "22510 1QS 7:31 conj x p\n",
      "22527 1QS 7:32 subs_a x p\n",
      "22531 1QS 7:32 subs_a x p\n",
      "22532 1QS 7:32 conj x p\n",
      "22544 1QS 7:33 subs_a x p\n",
      "22573 1QS 7:33 subs_c p x\n",
      "22574 1QS 7:33 verb_c p x\n",
      "22578 1QS 7:33 subs_a x p\n",
      "22583 1QS 7:33 verb_c p x\n",
      "22586 1QS 7:33 verb_c p x\n",
      "22589 1QS 7:33 verb_c p x\n",
      "22603 1QS 7:34 prps x p\n",
      "22625 1QS 7:35 verb_c x p\n",
      "22636 1QS 7:35 verb_a x p\n",
      "22637 1QS 7:35 conj x p\n",
      "22660 1QS 7:37 prps p x\n",
      "22661 1QS 7:37 conj p x\n",
      "22674 1QS 7:38 subs_a x p\n",
      "22683 1QS 7:39 verb_c x p\n",
      "22693 1QS 7:39 subs_a x p\n",
      "22694 1QS 7:39 conj x p\n",
      "22701 1QS 7:39 subs_a x p\n",
      "22702 1QS 7:39 conj x p\n",
      "22704 1QS 7:39 subs_a x p\n",
      "22735 1QS 7:40 subs_a x p\n",
      "22736 1QS 7:40 conj x p\n",
      "22758 1QS 7:42 prps x p\n",
      "22760 1QS 7:42 prps x p\n",
      "22762 1QS 7:42 prps x p\n",
      "22764 1QS 7:42 verb_c x p\n",
      "22769 1QS 7:42 subs_a p x\n",
      "22770 1QS 7:42 conj p x\n",
      "22771 1QS 7:42 verb_c p x\n",
      "22787 1QS 7:44 subs_a p x\n",
      "22804 1QS 7:45 subs_c x p\n",
      "22896 1QS 7:50 subs_a x p\n",
      "22897 1QS 7:50 conj x p\n",
      "22968 1QS 7:54 subs_c x p\n",
      "22969 1QS 7:54 art x p\n",
      "22976 1QS 7:55 subs_c p x\n",
      "23005 1QS 7:55 verb_c x p\n",
      "23006 1QS 7:55 prps x p\n",
      "23051 1QS 7:60 subs_a p x\n",
      "23052 1QS 7:60 subs_a x p\n",
      "23062 1QS 7:61 subs_a x p\n",
      "23063 1QS 7:61 conj x p\n"
     ]
    }
   ],
   "source": [
    "for error in bible_section_dss:\n",
    "    print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
